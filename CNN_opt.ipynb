{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Importing the libraries"
      ],
      "metadata": {
        "id": "tS198M_w4OVl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9kgTwCtm4Kcv"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import seaborn as sns\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D, MaxPooling2D, Dropout, Flatten"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initialize train and test dataset"
      ],
      "metadata": {
        "id": "5gUBRQDI4oyV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Making the grids"
      ],
      "metadata": {
        "id": "Xx0uW7XO4r37"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "targetdata =  #data that we want to predict\n",
        "dim_targetdata =  #number of values that need to be predicted per input point\n",
        "grids =  #input igures\n",
        "dimgrids =  #dimensionality of the grids\n",
        "train_size =  #number of datapoints in the train dataset\n",
        "test_size =  #number of datapoints in the test dataset\n",
        "dim_latent_space =  #dimension of the latent space representation"
      ],
      "metadata": {
        "id": "oMvdus7R4lkG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "indexen = range(len(targetdata))\n",
        "X_trainges, X_testges, indexen_trainges_final, indexen_testges_final = train_test_split(grids, indexen, train_size=train_size, test_size=test_size, random_state=333)\n",
        "training_yges = targetdata[indexen_trainges_final]\n",
        "y_testges = targetdata[indexen_testges_final]"
      ],
      "metadata": {
        "id": "63NlF3555Hmm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X1 = np.array(X_trainges).reshape(len(X_trainges), dimgrids,dimgrids,1)\n",
        "Y = np.array(training_yges).reshape(len(training_yges), dim_targetdata)"
      ],
      "metadata": {
        "id": "Gf8pc7uw5Jo2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optimization for the number of layers"
      ],
      "metadata": {
        "id": "PEZswRZz5PP1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nets = 3\n",
        "model = [0] *nets\n",
        "\n",
        "for j in range(3):\n",
        "    model[j] = Sequential()\n",
        "    model[j].add(Conv2D(4, kernel_size=(9,9), activation='relu', input_shape=(66, 66, 1)))\n",
        "    model[j].add(MaxPooling2D(2, 2))\n",
        "    if j>0:\n",
        "        model[j].add(Conv2D(8, kernel_size=(9,9), activation='relu'))\n",
        "        model[j].add(MaxPooling2D(2, 2))\n",
        "    if j>1:\n",
        "        model[j].add(Conv2D(16, kernel_size=(9,9), activation='relu'))\n",
        "        model[j].add(MaxPooling2D(2, 2))\n",
        "    model[j].add(Flatten())\n",
        "    model[j].add(Dense(256, activation='relu'))\n",
        "    model[j].add(Dense(99))\n",
        "    model[j].compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "WHGl65wL75fF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hist0 = model[0].fit(X1, Y, epochs=50,validation_split=0.2,batch_size=32)\n",
        "hist1 = model[1].fit(X1, Y, epochs=50,validation_split=0.2,batch_size=32)\n",
        "hist2 = model[2].fit(X1, Y, epochs=50,validation_split=0.2,batch_size=32)"
      ],
      "metadata": {
        "id": "pIX2oOrA78AA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(hist0.history['accuracy'])\n",
        "plt.plot(hist1.history['accuracy'])\n",
        "plt.plot(hist2.history['accuracy'])\n",
        "plt.legend(['1 layer','2 layers','3 layers'])"
      ],
      "metadata": {
        "id": "I7vE0p5W7-RT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(hist0.history['loss'])\n",
        "plt.plot(hist1.history['loss'])\n",
        "plt.plot(hist2.history['loss'])\n",
        "plt.legend(['1 layer','2 layers','3 layers'])"
      ],
      "metadata": {
        "id": "_6Gef68A8CQb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Number of feature maps"
      ],
      "metadata": {
        "id": "v62iKCXk8Glp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "go on with the number of layers that was the best in the previous step"
      ],
      "metadata": {
        "id": "9Seu4KZf8Ipv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nets = 6\n",
        "model = [0] *nets\n",
        "for j in range(6):\n",
        "    model[j] = Sequential()\n",
        "    model[j].add(Conv2D(j*2+2,kernel_size=5,activation='relu',input_shape=(66, 66, 1)))\n",
        "    model[j].add(MaxPooling2D(2, 2))\n",
        "    model[j].add(Flatten())\n",
        "    model[j].add(Dense(256, activation='relu'))\n",
        "    model[j].add(Dense(99))\n",
        "    model[j].compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "5qNOMe7K8GMX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hist0 = model[0].fit(X1, Y, epochs=20,validation_split=0.2,batch_size=32)\n",
        "hist1 = model[1].fit(X1, Y, epochs=20,validation_split=0.2,batch_size=32)\n",
        "hist2 = model[2].fit(X1, Y, epochs=20,validation_split=0.2,batch_size=32)\n",
        "hist3 = model[3].fit(X1, Y, epochs=20,validation_split=0.2,batch_size=32)\n",
        "hist4 = model[4].fit(X1, Y, epochs=20,validation_split=0.2,batch_size=32)\n",
        "hist5 = model[5].fit(X1, Y, epochs=20,validation_split=0.2,batch_size=32)"
      ],
      "metadata": {
        "id": "e_VKdIaf8Onb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(hist0.history['accuracy'])\n",
        "plt.plot(hist1.history['accuracy'])\n",
        "plt.plot(hist2.history['accuracy'])\n",
        "plt.plot(hist3.history['accuracy'])\n",
        "plt.plot(hist4.history['accuracy'])\n",
        "plt.plot(hist5.history['accuracy'])\n",
        "plt.legend(['1 feature map','3 feature maps','5 feature maps','7 feature maps','9 feature maps','11 feature maps']) "
      ],
      "metadata": {
        "id": "serTs4058QjF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(hist0.history['loss'])\n",
        "plt.plot(hist1.history['loss'])\n",
        "plt.plot(hist2.history['loss'])\n",
        "plt.plot(hist3.history['loss'])\n",
        "plt.plot(hist4.history['loss'])\n",
        "plt.plot(hist5.history['loss'])\n",
        "plt.legend(['1 feature map','3 feature maps','5 feature maps','7 feature maps','9 feature maps','11 feature maps']) "
      ],
      "metadata": {
        "id": "kL-Y7LL78UjZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How big is the first dense layer"
      ],
      "metadata": {
        "id": "2r4sgo3r8qe0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Go on with the number of layers and number of feature maps that was the best in the previous steps"
      ],
      "metadata": {
        "id": "VNlY-Brn8Yhm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nets = 8\n",
        "model = [0] *nets\n",
        "\n",
        "for j in range(8):\n",
        "    model[j] = Sequential()\n",
        "    model[j].add(Conv2D(9,kernel_size=5,activation='relu',input_shape=(66, 66, 1)))\n",
        "    model[j].add(MaxPooling2D(2, 2))\n",
        "    model[j].add(Flatten())\n",
        "    if j>0:\n",
        "        model[j].add(Dense(2**(j+4), activation='relu'))\n",
        "    model[j].add(Dense(99))\n",
        "    model[j].compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "p21PD34O8eHe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hist0 = model[0].fit(X1, Y, epochs=50,validation_split=0.2,batch_size=32)\n",
        "hist1 = model[1].fit(X1, Y, epochs=50,validation_split=0.2,batch_size=32)\n",
        "hist2 = model[2].fit(X1, Y, epochs=50,validation_split=0.2,batch_size=32)\n",
        "hist3 = model[3].fit(X1, Y, epochs=50,validation_split=0.2,batch_size=32)\n",
        "hist4 = model[4].fit(X1, Y, epochs=50,validation_split=0.2,batch_size=32)\n",
        "hist5 = model[5].fit(X1, Y, epochs=50,validation_split=0.2,batch_size=32)\n",
        "hist6 = model[6].fit(X1, Y, epochs=50,validation_split=0.2,batch_size=32)\n",
        "hist7 = model[7].fit(X1, Y, epochs=50,validation_split=0.2,batch_size=32)"
      ],
      "metadata": {
        "id": "k-7gRmll8g8P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.semilogy(hist0.history['accuracy'])\n",
        "plt.semilogy(hist1.history['accuracy'])\n",
        "plt.semilogy(hist2.history['accuracy'])\n",
        "plt.semilogy(hist3.history['accuracy'])\n",
        "plt.semilogy(hist4.history['accuracy'])\n",
        "plt.semilogy(hist5.history['accuracy'])\n",
        "plt.semilogy(hist6.history['accuracy'])\n",
        "plt.semilogy(hist7.history['accuracy'])\n",
        "plt.legend(['dim hidden state = 16','dim hidden state = 32','dim hidden state = 64','dim hidden state = 128 ','dim hidden state = 256','dim hidden state = 512','dim hidden state = 1024','dim hidden state = 2048']) "
      ],
      "metadata": {
        "id": "q3-jk-WF8lwW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.semilogy(hist0.history['loss'])\n",
        "plt.semilogy(hist1.history['loss'])\n",
        "plt.semilogy(hist2.history['loss'])\n",
        "plt.semilogy(hist3.history['loss'])\n",
        "plt.semilogy(hist4.history['loss'])\n",
        "plt.semilogy(hist5.history['loss'])\n",
        "plt.semilogy(hist6.history['loss'])\n",
        "plt.semilogy(hist7.history['loss'])\n",
        "plt.legend(['dim hidden state = 16','dim hidden state = 32','dim hidden state = 64','dim hidden state = 128 ','dim hidden state = 256','dim hidden state = 512','dim hidden state = 1024','dim hidden state = 2048']) "
      ],
      "metadata": {
        "id": "j7uwpY1F8m5z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dropout layer"
      ],
      "metadata": {
        "id": "bVcls8PK8xuD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Go on with the number of layers, number of feature maps and size of the dense layer that was the best in the previous steps"
      ],
      "metadata": {
        "id": "Q9GJlH5g9dYT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nets = 8\n",
        "model = [0] *nets\n",
        "\n",
        "for j in range(8):\n",
        "    model[j] = Sequential()\n",
        "    model[j].add(Conv2D(9,kernel_size=5,activation='relu',input_shape=(66, 66, 1)))\n",
        "    model[j].add(MaxPooling2D(2, 2))\n",
        "    model[j].add(Flatten())\n",
        "    model[j].add(Dense(512, activation='relu'))\n",
        "    model[j].add(Dropout(j*0.1))\n",
        "    model[j].add(Dense(99))\n",
        "    model[j].compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "0Sxt1LNq8zV2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hist0 = model[0].fit(X1, Y, epochs=100,validation_split=0.2,batch_size=32)\n",
        "hist1 = model[1].fit(X1, Y, epochs=100,validation_split=0.2,batch_size=32)\n",
        "hist2 = model[2].fit(X1, Y, epochs=100,validation_split=0.2,batch_size=32)\n",
        "hist3 = model[3].fit(X1, Y, epochs=100,validation_split=0.2,batch_size=32)\n",
        "hist4 = model[4].fit(X1, Y, epochs=100,validation_split=0.2,batch_size=32)\n",
        "hist5 = model[5].fit(X1, Y, epochs=100,validation_split=0.2,batch_size=32)\n",
        "hist6 = model[6].fit(X1, Y, epochs=100,validation_split=0.2,batch_size=32)\n",
        "hist7 = model[7].fit(X1, Y, epochs=100,validation_split=0.2,batch_size=32)"
      ],
      "metadata": {
        "id": "Js5Uip4q81ZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.semilogy(hist0.history['accuracy'])\n",
        "plt.semilogy(hist1.history['accuracy'])\n",
        "plt.semilogy(hist2.history['accuracy'])\n",
        "plt.semilogy(hist3.history['accuracy'])\n",
        "plt.semilogy(hist4.history['accuracy'])\n",
        "plt.semilogy(hist5.history['accuracy'])\n",
        "plt.semilogy(hist6.history['accuracy'])\n",
        "plt.semilogy(hist7.history['accuracy'])\n",
        "plt.legend(['dropout = 0','dropout = 0.1','dropout = 0.2','dropout = 0.3','dropout = 0.4','dropout = 0.5','dropout = 0.6','dropout = 0.7']) "
      ],
      "metadata": {
        "id": "Syjrufr-83yA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.semilogy(hist0.history['loss'])\n",
        "plt.semilogy(hist1.history['loss'])\n",
        "plt.semilogy(hist2.history['loss'])\n",
        "plt.semilogy(hist3.history['loss'])\n",
        "plt.semilogy(hist4.history['loss'])\n",
        "plt.semilogy(hist5.history['loss'])\n",
        "plt.semilogy(hist6.history['loss'])\n",
        "plt.semilogy(hist7.history['loss'])\n",
        "plt.legend(['dropout = 0','dropout = 0.1','dropout = 0.2','dropout = 0.3','dropout = 0.4','dropout = 0.5','dropout = 0.6','dropout = 0.7']) "
      ],
      "metadata": {
        "id": "MLXTc05a88zS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Kernel_size"
      ],
      "metadata": {
        "id": "Kh3Q0Nd389SC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Go on with the number of layers, number of feature maps, size of the dense layer and dropout rate that was the best in the previous steps"
      ],
      "metadata": {
        "id": "5jQJjGnc9i20"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nets = 8\n",
        "model = [0] *nets\n",
        "\n",
        "for j in range(8):\n",
        "    model[j] = Sequential()\n",
        "    model[j].add(Conv2D(9,kernel_size=j+5,activation='relu',input_shape=(66, 66, 1)))\n",
        "    model[j].add(MaxPooling2D(2, 2))\n",
        "    model[j].add(Flatten())\n",
        "    model[j].add(Dense(512, activation='relu'))\n",
        "    model[j].add(Dropout(0))\n",
        "    model[j].add(Dense(99))\n",
        "    model[j].compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "3rGExZ7V9Ck0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hist0 = model[0].fit(X1, Y, epochs=100,validation_split=0.2,batch_size=32)\n",
        "hist1 = model[1].fit(X1, Y, epochs=100,validation_split=0.2,batch_size=32)\n",
        "hist2 = model[2].fit(X1, Y, epochs=100,validation_split=0.2,batch_size=32)\n",
        "hist3 = model[3].fit(X1, Y, epochs=100,validation_split=0.2,batch_size=32)\n",
        "hist4 = model[4].fit(X1, Y, epochs=100,validation_split=0.2,batch_size=32)\n",
        "hist5 = model[5].fit(X1, Y, epochs=100,validation_split=0.2,batch_size=32)\n",
        "hist6 = model[6].fit(X1, Y, epochs=100,validation_split=0.2,batch_size=32)\n",
        "hist7 = model[7].fit(X1, Y, epochs=100,validation_split=0.2,batch_size=32)"
      ],
      "metadata": {
        "id": "TqmDjG6F9Fp-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.semilogy(hist0.history['accuracy'])\n",
        "plt.semilogy(hist1.history['accuracy'])\n",
        "plt.semilogy(hist2.history['accuracy'])\n",
        "plt.semilogy(hist3.history['accuracy'])\n",
        "plt.semilogy(hist4.history['accuracy'])\n",
        "plt.semilogy(hist5.history['accuracy'])\n",
        "plt.semilogy(hist6.history['accuracy'])\n",
        "plt.semilogy(hist7.history['accuracy'])\n",
        "plt.legend(['kernel size = 5','kernel size = 6','kernel size = 7','kernel size = 8','kernel size = 9','kernel size = 10','kernel size = 11','kernel size = 12']) "
      ],
      "metadata": {
        "id": "6sioPFSb9H77"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.semilogy(hist0.history['loss'])\n",
        "plt.semilogy(hist1.history['loss'])\n",
        "plt.semilogy(hist2.history['loss'])\n",
        "plt.semilogy(hist3.history['loss'])\n",
        "plt.semilogy(hist4.history['loss'])\n",
        "plt.semilogy(hist5.history['loss'])\n",
        "plt.semilogy(hist6.history['loss'])\n",
        "plt.semilogy(hist7.history['loss'])\n",
        "plt.legend(['kernel size = 5','kernel size = 6','kernel size = 7','kernel size = 8','kernel size = 9','kernel size = 10','kernel size = 11','kernel size = 12']) "
      ],
      "metadata": {
        "id": "QZm2RemK9YEu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}