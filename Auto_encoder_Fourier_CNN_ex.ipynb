{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import the libraries that are needed"
      ],
      "metadata": {
        "id": "gN35Ae16c7Z7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6MK6jzNccegg",
        "outputId": "87767598-7e8a-4ddc-c7d2-ef8147f59916"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.9.2\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import seaborn as sns\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import sklearn\n",
        "from sklearn import preprocessing\n",
        "import scipy as sc\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D, MaxPooling2D, Dropout, Flatten"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#initialize the train and test data set + perform the fourier transformation"
      ],
      "metadata": {
        "id": "IBHRRToidq-L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "making the grids"
      ],
      "metadata": {
        "id": "KzARYSttQRPU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "modinfo = np.loadtxt('modinfo.csv',dtype=float,delimiter=',')"
      ],
      "metadata": {
        "id": "tT1hwi-iQSjZ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "allegrids = []\n",
        "for ii in range(len(modinfo)):\n",
        "  grid = np.zeros((200,200))\n",
        "  for kk in range(np.int(np.round(modinfo[ii,0]))):\n",
        "      for jj in range(200):\n",
        "        grid[kk,jj] = modinfo[ii,2]\n",
        "\n",
        "  for kk in range(np.int(np.round(modinfo[ii,0])),200):\n",
        "    for jj in range(200):\n",
        "      grid[kk,jj] = modinfo[ii,3]\n",
        "\n",
        "  tanding = np.abs(np.round(np.tan(np.deg2rad(modinfo[ii,1])) * 200/2))\n",
        "  for mm in range(100,200):\n",
        "    for kk in range(max(0,np.int(-(mm-100)*tanding/100 + np.int(np.round(modinfo[ii,0])))),np.int(np.round(modinfo[ii,0]))):\n",
        "      grid[kk,mm] =  modinfo[ii,3]\n",
        "\n",
        "  for mm in range(0,100):\n",
        "    for kk in range(np.int(np.round(modinfo[ii,0])),min(200,np.int(-(mm-100)*tanding/100 + np.int(np.round(modinfo[ii,0]))))):\n",
        "      grid[kk,mm] =  modinfo[ii,2]\n",
        "  allegrids.append(grid)\n",
        "#reduce dimension (for minimal RAM usage)\n",
        "herschaalmod = Sequential()\n",
        "herschaalmod.add(MaxPooling2D(3, 3))\n",
        "herschaalmod.compile(optimizer='adam', loss='binary_crossentropy',metrics=['accuracy'])\n",
        "X = np.array(allegrids).reshape(len(allegrids), 200,200,1)\n",
        "kleinegrids = herschaalmod.predict(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NDE1xzjrQU5S",
        "outputId": "9e2b4aae-109e-44e0-9862-c75b9de49b7b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  after removing the cwd from sys.path.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:14: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:18: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "230/230 [==============================] - 6s 24ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "targetdata = np.loadtxt('verschilgeschaald.csv',dtype=float,delimiter=',') #data that we want to predict\n",
        "dim_targetdata = 99 #number of values that need to be predicted per input point\n",
        "modinfo = np.loadtxt('modinfo.csv',dtype=float,delimiter=',') \n",
        "alleLFs = np.loadtxt('alleLFs.csv',dtype=float,delimiter=',') \n",
        "features = []\n",
        "for ii in range(len(modinfo)): \n",
        "    rij = modinfo[ii]\n",
        "    toetevoegen = np.concatenate([rij, alleLFs[ii]])\n",
        "    features.append(toetevoegen)\n",
        "features = np.array(features) #input data for the model\n",
        "dim_modinfo = 4+99 #number of input parameters per input point\n",
        "train_size = 6300 #number of datapoints in the train dataset\n",
        "test_size = 1000 #number of datapoints in the test dataset\n",
        "dim_latent_space = 5 #dimension of the latent space representation\n",
        "num_feature_maps = 9 #number of feature maps in the convolutional layer used to predict the parameters (optimization might yield different parameters for the fouriercomponents and the hidden states but that is not taken into account here)\n",
        "kernel_size = 6 #dimension of the kernel in the convolutional layer used to predict the parameters(kernel_size x kernel_size) (optimization might yield different parameters for the fouriercomponents and the hidden states but that is not taken into account here)\n",
        "dense_layer_size = 2048 #number of units for the dense layer (optimization might yield different parameters for the fouriercomponents and the hidden states but that is not taken into account here)\n",
        "grids = kleinegrids #input figures\n",
        "dimgrids = 66 #dimensionality of the grids"
      ],
      "metadata": {
        "id": "v2lisNhXdxst"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#perform the fourier transform\n",
        "verschilfour = []\n",
        "for ii in range(len(targetdata)):\n",
        "  verschilfour.append(sc.fft.fft(targetdata[ii]))\n",
        "verschilfour = np.array(verschilfour)\n",
        "#split in real and imaginary part\n",
        "re = np.real(verschilfour)\n",
        "im = np.imag(verschilfour)\n",
        "topred = []\n",
        "for ii in range(len(re)):\n",
        "  tijdsding = []\n",
        "  for jj in range(len(re[0])):\n",
        "    elnt = [re[ii,jj],im[ii,jj]]\n",
        "    tijdsding.append(elnt)\n",
        "  topred.append(tijdsding)\n",
        "topred = np.array(topred)\n",
        "\n",
        "indexen = range(len(verschilfour))\n",
        "\n",
        "#split in train and test data set\n",
        "X_trainges, X_testges, indexen_trainges_final, indexen_testges_final = train_test_split(features, indexen, train_size=train_size, test_size=test_size, random_state=333)\n",
        "training_yges = topred[indexen_trainges_final]\n",
        "y_testges = topred[indexen_testges_final]"
      ],
      "metadata": {
        "id": "MSxgPAf7c_qK"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exploit symmetry of the fouriertransform"
      ],
      "metadata": {
        "id": "n-Z65cUhfIwp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "redeel = (topred[:,:,0]).copy()\n",
        "for ii in range(int((dim_targetdata-1)/2),dim_targetdata):\n",
        "  redeel[:,ii] = topred[:,int((dim_targetdata+1)/2-abs((dim_targetdata-1)/2-ii)),0]"
      ],
      "metadata": {
        "id": "_sCyJZ9ZfOL6"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imdeel = (topred[:,:,1]).copy()\n",
        "for ii in range(int((dim_targetdata-1)/2),dim_targetdata):\n",
        "  imdeel[:,ii] = -topred[:,int((dim_targetdata+1)/2-abs((dim_targetdata-1)/2-ii)),1]"
      ],
      "metadata": {
        "id": "5bh_Rq_qfuk4"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define the fouriercomponents that will be used for the prediction"
      ],
      "metadata": {
        "id": "kFQW9UcVf8QC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "relim = [5,95,99] #[x,y,z]: fouriercomponents from 0 to x and from y to z are taken into account for the real part with the last ones mirrored from the first ones\n",
        "imlim = [4,96,99] #[x,y,z]: fouriercomponents from 0 to x and from y to z are taken into account for the imag part with the last ones mirrored from the first ones"
      ],
      "metadata": {
        "id": "btkWmqTvgFbF"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predfour = topred.copy()\n",
        "for jj in range(len(predfour)):\n",
        "  for ii in range(imlim[0],imlim[1]):\n",
        "    predfour[jj][ii][1] = 0\n",
        "  for ii in range(relim[0],relim[1]):\n",
        "    predfour[jj][ii][0] = 0\n",
        "  for ii in range(imlim[1],imlim[2]):\n",
        "    predfour[jj][ii][1] = -topred[jj,np.int((dim_targetdata+1)/2-abs((dim_targetdata-1)/2-ii)),1]\n",
        "  for ii in range(relim[1],relim[2]):\n",
        "    predfour[jj][ii][0] = topred[jj,np.int((dim_targetdata+1)/2-abs((dim_targetdata-1)/2-ii)),0]  "
      ],
      "metadata": {
        "id": "_ISSR_MmgBTx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2088b477-1c48-4ec3-f6c4-d3ce8b562dc0"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # Remove the CWD from sys.path while we load stuff.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prediction based on the limited number of fourier components"
      ],
      "metadata": {
        "id": "QpNmnNKJgbJu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pred = []\n",
        "for ii in range(len(predfour)):\n",
        "  tijdsvector = []\n",
        "  for jj in range(len(predfour[0])):\n",
        "    tijdsvector.append(complex(predfour[ii,jj,0],predfour[ii,jj,1]))\n",
        "  pred.append(tijdsvector)\n",
        "predictie = np.real(sc.fft.ifft(pred))"
      ],
      "metadata": {
        "id": "XiqBSnDegeXl"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Use neural network to estimate the difference between the fourierprediction and the real values"
      ],
      "metadata": {
        "id": "IsfpWqB2gpCB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "errfour = []\n",
        "for ii in range(len(targetdata)):\n",
        "  errfour.append(targetdata[ii]-predictie[ii])"
      ],
      "metadata": {
        "id": "cUerv673gt8x"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "split in train and test dataset"
      ],
      "metadata": {
        "id": "cFpXXkD1hDGK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "indexenfourr = range(len(errfour))\n",
        "errfour_train, errfour_test, indexen_trainges, indexen_testges = train_test_split(errfour, indexenfourr, train_size=train_size, test_size=test_size, random_state=333)"
      ],
      "metadata": {
        "id": "UXbYVuGxhFIg"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "make and train the neural network"
      ],
      "metadata": {
        "id": "QcXtsD1dhLdq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.array(errfour_train).reshape(len(errfour_train), dim_targetdata)\n",
        "params_input = keras.Input(shape=(dim_targetdata))\n",
        "encoder = tf.keras.layers.Dense(dim_latent_space,activation = 'tanh')(params_input)\n",
        "decoder = tf.keras.layers.Dense(dim_targetdata)(encoder)\n",
        "modelauto = keras.Model(inputs=params_input,outputs=decoder)\n",
        "modelauto.compile(optimizer='adam', loss='mse')\n",
        "modelauto.summary()\n",
        "hist = modelauto.fit(X,X,epochs=500,validation_split=0.2,batch_size=32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F850WGw5hLJF",
        "outputId": "fb5b558a-7277-4803-f1aa-860eef73d5ce"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 99)]              0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 5)                 500       \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 99)                594       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,094\n",
            "Trainable params: 1,094\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/500\n",
            "158/158 [==============================] - 2s 8ms/step - loss: 4.1109 - val_loss: 0.0012\n",
            "Epoch 2/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 4.1089 - val_loss: 0.0012\n",
            "Epoch 3/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 4.1073 - val_loss: 0.0012\n",
            "Epoch 4/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 4.1059 - val_loss: 0.0012\n",
            "Epoch 5/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 4.1043 - val_loss: 0.0012\n",
            "Epoch 6/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 4.1027 - val_loss: 0.0012\n",
            "Epoch 7/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 4.1011 - val_loss: 0.0012\n",
            "Epoch 8/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 4.0995 - val_loss: 0.0012\n",
            "Epoch 9/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 4.0978 - val_loss: 0.0012\n",
            "Epoch 10/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 4.0962 - val_loss: 0.0013\n",
            "Epoch 11/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 4.0945 - val_loss: 0.0013\n",
            "Epoch 12/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 4.0929 - val_loss: 0.0013\n",
            "Epoch 13/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 4.0912 - val_loss: 0.0013\n",
            "Epoch 14/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 4.0895 - val_loss: 0.0014\n",
            "Epoch 15/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 4.0878 - val_loss: 0.0014\n",
            "Epoch 16/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 4.0899 - val_loss: 0.0014\n",
            "Epoch 17/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 4.0846 - val_loss: 0.0015\n",
            "Epoch 18/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 4.0829 - val_loss: 0.0015\n",
            "Epoch 19/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 4.0812 - val_loss: 0.0015\n",
            "Epoch 20/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 4.0795 - val_loss: 0.0015\n",
            "Epoch 21/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 4.0779 - val_loss: 0.0015\n",
            "Epoch 22/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 4.0761 - val_loss: 0.0015\n",
            "Epoch 23/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 4.0744 - val_loss: 0.0015\n",
            "Epoch 24/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 4.0726 - val_loss: 0.0015\n",
            "Epoch 25/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 4.0709 - val_loss: 0.0016\n",
            "Epoch 26/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 4.0692 - val_loss: 0.0015\n",
            "Epoch 27/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 4.0676 - val_loss: 0.0015\n",
            "Epoch 28/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 4.0659 - val_loss: 0.0015\n",
            "Epoch 29/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 4.0642 - val_loss: 0.0015\n",
            "Epoch 30/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 4.0625 - val_loss: 0.0015\n",
            "Epoch 31/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 4.0610 - val_loss: 0.0015\n",
            "Epoch 32/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 4.0794 - val_loss: 0.0014\n",
            "Epoch 33/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 4.0777 - val_loss: 0.0014\n",
            "Epoch 34/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 4.0984 - val_loss: 0.0015\n",
            "Epoch 35/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 4.0967 - val_loss: 0.0015\n",
            "Epoch 36/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 4.0937 - val_loss: 0.0015\n",
            "Epoch 37/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 4.1164 - val_loss: 0.0016\n",
            "Epoch 38/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 4.0912 - val_loss: 0.0016\n",
            "Epoch 39/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 4.0897 - val_loss: 0.0016\n",
            "Epoch 40/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 4.0881 - val_loss: 0.0016\n",
            "Epoch 41/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 4.0866 - val_loss: 0.0016\n",
            "Epoch 42/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 4.0850 - val_loss: 0.0016\n",
            "Epoch 43/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 4.0835 - val_loss: 0.0016\n",
            "Epoch 44/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 4.0819 - val_loss: 0.0016\n",
            "Epoch 45/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 4.0803 - val_loss: 0.0016\n",
            "Epoch 46/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 4.0786 - val_loss: 0.0016\n",
            "Epoch 47/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 4.0770 - val_loss: 0.0016\n",
            "Epoch 48/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 4.0753 - val_loss: 0.0015\n",
            "Epoch 49/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 4.0744 - val_loss: 0.0015\n",
            "Epoch 50/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 4.0722 - val_loss: 0.0014\n",
            "Epoch 51/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 4.0705 - val_loss: 0.0014\n",
            "Epoch 52/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 4.0689 - val_loss: 0.0014\n",
            "Epoch 53/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 4.0672 - val_loss: 0.0014\n",
            "Epoch 54/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 4.0655 - val_loss: 0.0013\n",
            "Epoch 55/500\n",
            "158/158 [==============================] - 0s 3ms/step - loss: 4.0639 - val_loss: 0.0013\n",
            "Epoch 56/500\n",
            "158/158 [==============================] - 0s 3ms/step - loss: 4.0621 - val_loss: 0.0013\n",
            "Epoch 57/500\n",
            "158/158 [==============================] - 1s 4ms/step - loss: 4.0605 - val_loss: 0.0013\n",
            "Epoch 58/500\n",
            "158/158 [==============================] - 1s 3ms/step - loss: 4.0588 - val_loss: 0.0013\n",
            "Epoch 59/500\n",
            "158/158 [==============================] - 1s 4ms/step - loss: 4.0570 - val_loss: 0.0012\n",
            "Epoch 60/500\n",
            "158/158 [==============================] - 1s 5ms/step - loss: 4.0554 - val_loss: 0.0012\n",
            "Epoch 61/500\n",
            "158/158 [==============================] - 0s 3ms/step - loss: 4.0537 - val_loss: 0.0012\n",
            "Epoch 62/500\n",
            "158/158 [==============================] - 1s 3ms/step - loss: 4.0520 - val_loss: 0.0012\n",
            "Epoch 63/500\n",
            "158/158 [==============================] - 1s 4ms/step - loss: 4.0475 - val_loss: 0.0012\n",
            "Epoch 64/500\n",
            "158/158 [==============================] - 1s 5ms/step - loss: 4.0458 - val_loss: 0.0012\n",
            "Epoch 65/500\n",
            "158/158 [==============================] - 0s 3ms/step - loss: 4.0442 - val_loss: 0.0011\n",
            "Epoch 66/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 4.0426 - val_loss: 0.0011\n",
            "Epoch 67/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 4.0409 - val_loss: 0.0011\n",
            "Epoch 68/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 4.0392 - val_loss: 0.0011\n",
            "Epoch 69/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 4.0376 - val_loss: 0.0011\n",
            "Epoch 70/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 4.0358 - val_loss: 0.0011\n",
            "Epoch 71/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 4.0342 - val_loss: 0.0011\n",
            "Epoch 72/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 4.0403 - val_loss: 0.0011\n",
            "Epoch 73/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 4.0386 - val_loss: 0.0010\n",
            "Epoch 74/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 4.0369 - val_loss: 0.0010\n",
            "Epoch 75/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 4.0352 - val_loss: 0.0010\n",
            "Epoch 76/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 4.0335 - val_loss: 0.0011\n",
            "Epoch 77/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 4.0270 - val_loss: 0.0010\n",
            "Epoch 78/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 4.0254 - val_loss: 0.0011\n",
            "Epoch 79/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 4.0237 - val_loss: 0.0011\n",
            "Epoch 80/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 4.0221 - val_loss: 0.0011\n",
            "Epoch 81/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 4.0681 - val_loss: 0.0013\n",
            "Epoch 82/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 4.0195 - val_loss: 0.0011\n",
            "Epoch 83/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 4.0178 - val_loss: 0.0011\n",
            "Epoch 84/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 4.0162 - val_loss: 0.0011\n",
            "Epoch 85/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 4.0145 - val_loss: 0.0011\n",
            "Epoch 86/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 4.0128 - val_loss: 0.0011\n",
            "Epoch 87/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 4.0113 - val_loss: 0.0011\n",
            "Epoch 88/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 4.0096 - val_loss: 0.0011\n",
            "Epoch 89/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 4.0080 - val_loss: 0.0010\n",
            "Epoch 90/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 4.0064 - val_loss: 0.0011\n",
            "Epoch 91/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 4.0048 - val_loss: 0.0010\n",
            "Epoch 92/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 4.0034 - val_loss: 0.0012\n",
            "Epoch 93/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 4.0017 - val_loss: 0.0011\n",
            "Epoch 94/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 4.0000 - val_loss: 0.0011\n",
            "Epoch 95/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 4.0088 - val_loss: 0.0011\n",
            "Epoch 96/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.9973 - val_loss: 0.0011\n",
            "Epoch 97/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.9956 - val_loss: 0.0011\n",
            "Epoch 98/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.9940 - val_loss: 0.0012\n",
            "Epoch 99/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.9924 - val_loss: 0.0011\n",
            "Epoch 100/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.9908 - val_loss: 0.0011\n",
            "Epoch 101/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.9891 - val_loss: 0.0011\n",
            "Epoch 102/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.9875 - val_loss: 0.0012\n",
            "Epoch 103/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.9860 - val_loss: 0.0011\n",
            "Epoch 104/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.9843 - val_loss: 0.0011\n",
            "Epoch 105/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.9827 - val_loss: 0.0011\n",
            "Epoch 106/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.9811 - val_loss: 0.0011\n",
            "Epoch 107/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.9795 - val_loss: 0.0011\n",
            "Epoch 108/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.9780 - val_loss: 0.0011\n",
            "Epoch 109/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.9764 - val_loss: 0.0010\n",
            "Epoch 110/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.9748 - val_loss: 0.0010\n",
            "Epoch 111/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.9732 - val_loss: 0.0010\n",
            "Epoch 112/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.9715 - val_loss: 0.0010\n",
            "Epoch 113/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.9699 - val_loss: 0.0010\n",
            "Epoch 114/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.9683 - val_loss: 0.0011\n",
            "Epoch 115/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.9668 - val_loss: 0.0010\n",
            "Epoch 116/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.9652 - val_loss: 0.0010\n",
            "Epoch 117/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.9636 - val_loss: 0.0010\n",
            "Epoch 118/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.9620 - val_loss: 0.0011\n",
            "Epoch 119/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.9604 - val_loss: 0.0011\n",
            "Epoch 120/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.9588 - val_loss: 0.0011\n",
            "Epoch 121/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.9572 - val_loss: 0.0012\n",
            "Epoch 122/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.9556 - val_loss: 0.0012\n",
            "Epoch 123/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.9541 - val_loss: 0.0012\n",
            "Epoch 124/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.9542 - val_loss: 0.0043\n",
            "Epoch 125/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.9812 - val_loss: 0.0036\n",
            "Epoch 126/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.9795 - val_loss: 0.0032\n",
            "Epoch 127/500\n",
            "158/158 [==============================] - 1s 4ms/step - loss: 3.9778 - val_loss: 0.0029\n",
            "Epoch 128/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.9762 - val_loss: 0.0033\n",
            "Epoch 129/500\n",
            "158/158 [==============================] - 1s 4ms/step - loss: 3.9746 - val_loss: 0.0036\n",
            "Epoch 130/500\n",
            "158/158 [==============================] - 1s 5ms/step - loss: 3.9729 - val_loss: 0.0032\n",
            "Epoch 131/500\n",
            "158/158 [==============================] - 1s 4ms/step - loss: 3.9712 - val_loss: 0.0032\n",
            "Epoch 132/500\n",
            "158/158 [==============================] - 0s 3ms/step - loss: 3.9696 - val_loss: 0.0035\n",
            "Epoch 133/500\n",
            "158/158 [==============================] - 0s 3ms/step - loss: 3.9680 - val_loss: 0.0035\n",
            "Epoch 134/500\n",
            "158/158 [==============================] - 0s 3ms/step - loss: 3.9664 - val_loss: 0.0040\n",
            "Epoch 135/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.9648 - val_loss: 0.0036\n",
            "Epoch 136/500\n",
            "158/158 [==============================] - 0s 3ms/step - loss: 3.9632 - val_loss: 0.0035\n",
            "Epoch 137/500\n",
            "158/158 [==============================] - 0s 3ms/step - loss: 4.0369 - val_loss: 0.0039\n",
            "Epoch 138/500\n",
            "158/158 [==============================] - 1s 3ms/step - loss: 4.0352 - val_loss: 0.0032\n",
            "Epoch 139/500\n",
            "158/158 [==============================] - 1s 3ms/step - loss: 4.0334 - val_loss: 0.0037\n",
            "Epoch 140/500\n",
            "158/158 [==============================] - 1s 5ms/step - loss: 4.0316 - val_loss: 0.0036\n",
            "Epoch 141/500\n",
            "158/158 [==============================] - 1s 4ms/step - loss: 4.0128 - val_loss: 0.0037\n",
            "Epoch 142/500\n",
            "158/158 [==============================] - 1s 3ms/step - loss: 4.0112 - val_loss: 0.0031\n",
            "Epoch 143/500\n",
            "158/158 [==============================] - 1s 5ms/step - loss: 4.0095 - val_loss: 0.0035\n",
            "Epoch 144/500\n",
            "158/158 [==============================] - 1s 4ms/step - loss: 4.0078 - val_loss: 0.0036\n",
            "Epoch 145/500\n",
            "158/158 [==============================] - 0s 3ms/step - loss: 4.0062 - val_loss: 0.0030\n",
            "Epoch 146/500\n",
            "158/158 [==============================] - 1s 5ms/step - loss: 4.0045 - val_loss: 0.0032\n",
            "Epoch 147/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 4.0029 - val_loss: 0.0029\n",
            "Epoch 148/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 4.0013 - val_loss: 0.0033\n",
            "Epoch 149/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.9996 - val_loss: 0.0030\n",
            "Epoch 150/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 4.0413 - val_loss: 0.0073\n",
            "Epoch 151/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.9972 - val_loss: 0.0063\n",
            "Epoch 152/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.9955 - val_loss: 0.0053\n",
            "Epoch 153/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.9938 - val_loss: 0.0045\n",
            "Epoch 154/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.9922 - val_loss: 0.0036\n",
            "Epoch 155/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.9905 - val_loss: 0.0031\n",
            "Epoch 156/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.9889 - val_loss: 0.0025\n",
            "Epoch 157/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.9878 - val_loss: 0.0022\n",
            "Epoch 158/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.9849 - val_loss: 0.0019\n",
            "Epoch 159/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.9835 - val_loss: 0.0018\n",
            "Epoch 160/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.9822 - val_loss: 0.0017\n",
            "Epoch 161/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.9807 - val_loss: 0.0017\n",
            "Epoch 162/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.9792 - val_loss: 0.0016\n",
            "Epoch 163/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.9778 - val_loss: 0.0016\n",
            "Epoch 164/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.9763 - val_loss: 0.0017\n",
            "Epoch 165/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.9747 - val_loss: 0.0017\n",
            "Epoch 166/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.9732 - val_loss: 0.0017\n",
            "Epoch 167/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.9717 - val_loss: 0.0018\n",
            "Epoch 168/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.9702 - val_loss: 0.0018\n",
            "Epoch 169/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.9687 - val_loss: 0.0019\n",
            "Epoch 170/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.9671 - val_loss: 0.0019\n",
            "Epoch 171/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.9656 - val_loss: 0.0020\n",
            "Epoch 172/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.9641 - val_loss: 0.0020\n",
            "Epoch 173/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.9625 - val_loss: 0.0020\n",
            "Epoch 174/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.9609 - val_loss: 0.0021\n",
            "Epoch 175/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.9594 - val_loss: 0.0021\n",
            "Epoch 176/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.9578 - val_loss: 0.0022\n",
            "Epoch 177/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.9562 - val_loss: 0.0022\n",
            "Epoch 178/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.9547 - val_loss: 0.0022\n",
            "Epoch 179/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.9531 - val_loss: 0.0021\n",
            "Epoch 180/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.9515 - val_loss: 0.0020\n",
            "Epoch 181/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.9499 - val_loss: 0.0020\n",
            "Epoch 182/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.9484 - val_loss: 0.0022\n",
            "Epoch 183/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.9468 - val_loss: 0.0020\n",
            "Epoch 184/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.9453 - val_loss: 0.0021\n",
            "Epoch 185/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.9437 - val_loss: 0.0020\n",
            "Epoch 186/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.9422 - val_loss: 0.0018\n",
            "Epoch 187/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.9407 - val_loss: 0.0018\n",
            "Epoch 188/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.9391 - val_loss: 0.0019\n",
            "Epoch 189/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.9375 - val_loss: 0.0016\n",
            "Epoch 190/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.9360 - val_loss: 0.0017\n",
            "Epoch 191/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.9344 - val_loss: 0.0017\n",
            "Epoch 192/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.9328 - val_loss: 0.0015\n",
            "Epoch 193/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.9313 - val_loss: 0.0016\n",
            "Epoch 194/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.9297 - val_loss: 0.0014\n",
            "Epoch 195/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.9282 - val_loss: 0.0015\n",
            "Epoch 196/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.9266 - val_loss: 0.0014\n",
            "Epoch 197/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.9251 - val_loss: 0.0019\n",
            "Epoch 198/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.9236 - val_loss: 0.0015\n",
            "Epoch 199/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.9220 - val_loss: 0.0015\n",
            "Epoch 200/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.9205 - val_loss: 0.0015\n",
            "Epoch 201/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.9190 - val_loss: 0.0015\n",
            "Epoch 202/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.9174 - val_loss: 0.0013\n",
            "Epoch 203/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.9165 - val_loss: 0.0038\n",
            "Epoch 204/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.9145 - val_loss: 0.0038\n",
            "Epoch 205/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.9129 - val_loss: 0.0035\n",
            "Epoch 206/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.9113 - val_loss: 0.0036\n",
            "Epoch 207/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.9098 - val_loss: 0.0034\n",
            "Epoch 208/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.9082 - val_loss: 0.0032\n",
            "Epoch 209/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.9066 - val_loss: 0.0034\n",
            "Epoch 210/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.9051 - val_loss: 0.0031\n",
            "Epoch 211/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.9035 - val_loss: 0.0038\n",
            "Epoch 212/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.9021 - val_loss: 0.0029\n",
            "Epoch 213/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.9004 - val_loss: 0.0029\n",
            "Epoch 214/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.8990 - val_loss: 0.0026\n",
            "Epoch 215/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.8975 - val_loss: 0.0026\n",
            "Epoch 216/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.8959 - val_loss: 0.0025\n",
            "Epoch 217/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.8944 - val_loss: 0.0024\n",
            "Epoch 218/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.8943 - val_loss: 0.0029\n",
            "Epoch 219/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.9639 - val_loss: 0.0023\n",
            "Epoch 220/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.9620 - val_loss: 0.0021\n",
            "Epoch 221/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.9604 - val_loss: 0.0018\n",
            "Epoch 222/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.9588 - val_loss: 0.0016\n",
            "Epoch 223/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.9571 - val_loss: 0.0015\n",
            "Epoch 224/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.9555 - val_loss: 0.0013\n",
            "Epoch 225/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.9539 - val_loss: 0.0011\n",
            "Epoch 226/500\n",
            "158/158 [==============================] - 0s 3ms/step - loss: 3.9522 - val_loss: 0.0011\n",
            "Epoch 227/500\n",
            "158/158 [==============================] - 0s 3ms/step - loss: 3.9506 - val_loss: 0.0011\n",
            "Epoch 228/500\n",
            "158/158 [==============================] - 1s 3ms/step - loss: 3.9490 - val_loss: 0.0011\n",
            "Epoch 229/500\n",
            "158/158 [==============================] - 1s 5ms/step - loss: 3.9474 - val_loss: 0.0011\n",
            "Epoch 230/500\n",
            "158/158 [==============================] - 1s 4ms/step - loss: 3.9462 - val_loss: 0.0010\n",
            "Epoch 231/500\n",
            "158/158 [==============================] - 1s 4ms/step - loss: 3.9443 - val_loss: 0.0010\n",
            "Epoch 232/500\n",
            "158/158 [==============================] - 1s 5ms/step - loss: 3.9427 - val_loss: 0.0010\n",
            "Epoch 233/500\n",
            "158/158 [==============================] - 1s 3ms/step - loss: 3.9411 - val_loss: 9.9802e-04\n",
            "Epoch 234/500\n",
            "158/158 [==============================] - 1s 4ms/step - loss: 3.9395 - val_loss: 0.0011\n",
            "Epoch 235/500\n",
            "158/158 [==============================] - 1s 5ms/step - loss: 3.9379 - val_loss: 9.9101e-04\n",
            "Epoch 236/500\n",
            "158/158 [==============================] - 1s 3ms/step - loss: 3.9364 - val_loss: 9.8859e-04\n",
            "Epoch 237/500\n",
            "158/158 [==============================] - 0s 3ms/step - loss: 3.9348 - val_loss: 9.8195e-04\n",
            "Epoch 238/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.9332 - val_loss: 9.6578e-04\n",
            "Epoch 239/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.9316 - val_loss: 9.8519e-04\n",
            "Epoch 240/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.9300 - val_loss: 9.6772e-04\n",
            "Epoch 241/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.9285 - val_loss: 0.0011\n",
            "Epoch 242/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.9270 - val_loss: 9.7129e-04\n",
            "Epoch 243/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.9256 - val_loss: 9.4370e-04\n",
            "Epoch 244/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.9239 - val_loss: 9.4667e-04\n",
            "Epoch 245/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.9224 - val_loss: 9.3938e-04\n",
            "Epoch 246/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.9208 - val_loss: 0.0010\n",
            "Epoch 247/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.9193 - val_loss: 0.0010\n",
            "Epoch 248/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.9177 - val_loss: 9.6176e-04\n",
            "Epoch 249/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.9163 - val_loss: 9.9128e-04\n",
            "Epoch 250/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.9147 - val_loss: 9.7221e-04\n",
            "Epoch 251/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.9132 - val_loss: 9.2982e-04\n",
            "Epoch 252/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.9116 - val_loss: 9.1818e-04\n",
            "Epoch 253/500\n",
            "158/158 [==============================] - 0s 3ms/step - loss: 3.9101 - val_loss: 0.0011\n",
            "Epoch 254/500\n",
            "158/158 [==============================] - 0s 3ms/step - loss: 3.9085 - val_loss: 9.1750e-04\n",
            "Epoch 255/500\n",
            "158/158 [==============================] - 0s 3ms/step - loss: 3.9069 - val_loss: 9.1459e-04\n",
            "Epoch 256/500\n",
            "158/158 [==============================] - 0s 3ms/step - loss: 3.9054 - val_loss: 9.0777e-04\n",
            "Epoch 257/500\n",
            "158/158 [==============================] - 0s 3ms/step - loss: 3.9039 - val_loss: 9.1129e-04\n",
            "Epoch 258/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.9024 - val_loss: 0.0012\n",
            "Epoch 259/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.9008 - val_loss: 9.0100e-04\n",
            "Epoch 260/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.8993 - val_loss: 9.5954e-04\n",
            "Epoch 261/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.8979 - val_loss: 8.9907e-04\n",
            "Epoch 262/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.8963 - val_loss: 9.1084e-04\n",
            "Epoch 263/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.8949 - val_loss: 9.3114e-04\n",
            "Epoch 264/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.8934 - val_loss: 9.1517e-04\n",
            "Epoch 265/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.8918 - val_loss: 0.0011\n",
            "Epoch 266/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.8903 - val_loss: 9.0036e-04\n",
            "Epoch 267/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.8888 - val_loss: 9.1066e-04\n",
            "Epoch 268/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.8872 - val_loss: 9.6491e-04\n",
            "Epoch 269/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.8857 - val_loss: 0.0011\n",
            "Epoch 270/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.8404 - val_loss: 9.2147e-04\n",
            "Epoch 271/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.8389 - val_loss: 8.8663e-04\n",
            "Epoch 272/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.8374 - val_loss: 9.0158e-04\n",
            "Epoch 273/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.8360 - val_loss: 9.1986e-04\n",
            "Epoch 274/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.8346 - val_loss: 8.8744e-04\n",
            "Epoch 275/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.8331 - val_loss: 8.9683e-04\n",
            "Epoch 276/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.8316 - val_loss: 0.0010\n",
            "Epoch 277/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.8303 - val_loss: 9.4887e-04\n",
            "Epoch 278/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.8288 - val_loss: 9.1319e-04\n",
            "Epoch 279/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.8274 - val_loss: 8.9331e-04\n",
            "Epoch 280/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.8259 - val_loss: 9.0741e-04\n",
            "Epoch 281/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.8244 - val_loss: 9.3876e-04\n",
            "Epoch 282/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.8231 - val_loss: 9.4576e-04\n",
            "Epoch 283/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.8216 - val_loss: 9.7881e-04\n",
            "Epoch 284/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.8201 - val_loss: 9.7854e-04\n",
            "Epoch 285/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.8188 - val_loss: 9.3100e-04\n",
            "Epoch 286/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.8173 - val_loss: 0.0015\n",
            "Epoch 287/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.9576 - val_loss: 0.0015\n",
            "Epoch 288/500\n",
            "158/158 [==============================] - 1s 5ms/step - loss: 3.9561 - val_loss: 0.0011\n",
            "Epoch 289/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.9073 - val_loss: 0.0010\n",
            "Epoch 290/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.9056 - val_loss: 0.0011\n",
            "Epoch 291/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.9040 - val_loss: 9.7764e-04\n",
            "Epoch 292/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.9025 - val_loss: 0.0012\n",
            "Epoch 293/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.9010 - val_loss: 9.8180e-04\n",
            "Epoch 294/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.8994 - val_loss: 0.0011\n",
            "Epoch 295/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.8979 - val_loss: 0.0012\n",
            "Epoch 296/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.8969 - val_loss: 0.0011\n",
            "Epoch 297/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.8948 - val_loss: 0.0011\n",
            "Epoch 298/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.8933 - val_loss: 0.0012\n",
            "Epoch 299/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.8918 - val_loss: 0.0014\n",
            "Epoch 300/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.8904 - val_loss: 0.0014\n",
            "Epoch 301/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.8888 - val_loss: 0.0012\n",
            "Epoch 302/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.8873 - val_loss: 0.0012\n",
            "Epoch 303/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.8862 - val_loss: 0.0021\n",
            "Epoch 304/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.8845 - val_loss: 0.0021\n",
            "Epoch 305/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.8830 - val_loss: 0.0018\n",
            "Epoch 306/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.8814 - val_loss: 0.0020\n",
            "Epoch 307/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.8801 - val_loss: 0.0011\n",
            "Epoch 308/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.8784 - val_loss: 0.0020\n",
            "Epoch 309/500\n",
            "158/158 [==============================] - 0s 3ms/step - loss: 3.8769 - val_loss: 0.0012\n",
            "Epoch 310/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.8753 - val_loss: 0.0013\n",
            "Epoch 311/500\n",
            "158/158 [==============================] - 1s 3ms/step - loss: 3.8737 - val_loss: 0.0014\n",
            "Epoch 312/500\n",
            "158/158 [==============================] - 1s 3ms/step - loss: 3.8722 - val_loss: 0.0017\n",
            "Epoch 313/500\n",
            "158/158 [==============================] - 1s 4ms/step - loss: 3.8707 - val_loss: 0.0017\n",
            "Epoch 314/500\n",
            "158/158 [==============================] - 1s 4ms/step - loss: 3.8692 - val_loss: 0.0016\n",
            "Epoch 315/500\n",
            "158/158 [==============================] - 1s 4ms/step - loss: 3.8677 - val_loss: 0.0018\n",
            "Epoch 316/500\n",
            "158/158 [==============================] - 0s 3ms/step - loss: 3.8662 - val_loss: 0.0017\n",
            "Epoch 317/500\n",
            "158/158 [==============================] - 1s 9ms/step - loss: 3.8648 - val_loss: 0.0017\n",
            "Epoch 318/500\n",
            "158/158 [==============================] - 1s 5ms/step - loss: 3.8633 - val_loss: 0.0018\n",
            "Epoch 319/500\n",
            "158/158 [==============================] - 1s 4ms/step - loss: 3.8618 - val_loss: 0.0017\n",
            "Epoch 320/500\n",
            "158/158 [==============================] - 1s 5ms/step - loss: 3.8603 - val_loss: 0.0018\n",
            "Epoch 321/500\n",
            "158/158 [==============================] - 1s 5ms/step - loss: 3.8589 - val_loss: 0.0018\n",
            "Epoch 322/500\n",
            "158/158 [==============================] - 1s 4ms/step - loss: 3.8574 - val_loss: 0.0018\n",
            "Epoch 323/500\n",
            "158/158 [==============================] - 0s 3ms/step - loss: 3.8560 - val_loss: 0.0021\n",
            "Epoch 324/500\n",
            "158/158 [==============================] - 0s 3ms/step - loss: 3.8545 - val_loss: 0.0023\n",
            "Epoch 325/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.8531 - val_loss: 0.0017\n",
            "Epoch 326/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.8515 - val_loss: 0.0020\n",
            "Epoch 327/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.8500 - val_loss: 0.0020\n",
            "Epoch 328/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.8485 - val_loss: 0.0015\n",
            "Epoch 329/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.8470 - val_loss: 0.0018\n",
            "Epoch 330/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.8456 - val_loss: 0.0027\n",
            "Epoch 331/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.8441 - val_loss: 0.0022\n",
            "Epoch 332/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.8427 - val_loss: 0.0015\n",
            "Epoch 333/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.8411 - val_loss: 0.0016\n",
            "Epoch 334/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.8396 - val_loss: 0.0018\n",
            "Epoch 335/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.8383 - val_loss: 9.9244e-04\n",
            "Epoch 336/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.8367 - val_loss: 0.0014\n",
            "Epoch 337/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.8353 - val_loss: 0.0013\n",
            "Epoch 338/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.8338 - val_loss: 0.0012\n",
            "Epoch 339/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.8323 - val_loss: 0.0013\n",
            "Epoch 340/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.8309 - val_loss: 0.0013\n",
            "Epoch 341/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.8294 - val_loss: 9.4705e-04\n",
            "Epoch 342/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.8279 - val_loss: 9.9123e-04\n",
            "Epoch 343/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.8265 - val_loss: 0.0013\n",
            "Epoch 344/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.8251 - val_loss: 9.3548e-04\n",
            "Epoch 345/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.8236 - val_loss: 9.9326e-04\n",
            "Epoch 346/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.8222 - val_loss: 9.5516e-04\n",
            "Epoch 347/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.8208 - val_loss: 9.5797e-04\n",
            "Epoch 348/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.8194 - val_loss: 9.7284e-04\n",
            "Epoch 349/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.8180 - val_loss: 0.0010\n",
            "Epoch 350/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.8165 - val_loss: 0.0013\n",
            "Epoch 351/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.8152 - val_loss: 9.2176e-04\n",
            "Epoch 352/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.8136 - val_loss: 9.2170e-04\n",
            "Epoch 353/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.8127 - val_loss: 9.3992e-04\n",
            "Epoch 354/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.8108 - val_loss: 0.0011\n",
            "Epoch 355/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.8094 - val_loss: 9.4981e-04\n",
            "Epoch 356/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.8078 - val_loss: 9.3509e-04\n",
            "Epoch 357/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.8064 - val_loss: 9.5511e-04\n",
            "Epoch 358/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.8049 - val_loss: 9.9132e-04\n",
            "Epoch 359/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.8035 - val_loss: 0.0010\n",
            "Epoch 360/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.8021 - val_loss: 0.0011\n",
            "Epoch 361/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.8007 - val_loss: 0.0012\n",
            "Epoch 362/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.7992 - val_loss: 0.0010\n",
            "Epoch 363/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.7979 - val_loss: 0.0015\n",
            "Epoch 364/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.7964 - val_loss: 0.0016\n",
            "Epoch 365/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.7953 - val_loss: 0.0016\n",
            "Epoch 366/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.7936 - val_loss: 0.0013\n",
            "Epoch 367/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.7922 - val_loss: 0.0012\n",
            "Epoch 368/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.7908 - val_loss: 0.0011\n",
            "Epoch 369/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.7894 - val_loss: 0.0012\n",
            "Epoch 370/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.7880 - val_loss: 0.0012\n",
            "Epoch 371/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.7866 - val_loss: 0.0013\n",
            "Epoch 372/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.7851 - val_loss: 0.0016\n",
            "Epoch 373/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.7839 - val_loss: 0.0013\n",
            "Epoch 374/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.7824 - val_loss: 0.0013\n",
            "Epoch 375/500\n",
            "158/158 [==============================] - 0s 3ms/step - loss: 3.7809 - val_loss: 0.0012\n",
            "Epoch 376/500\n",
            "158/158 [==============================] - 0s 3ms/step - loss: 3.7798 - val_loss: 0.0015\n",
            "Epoch 377/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.7781 - val_loss: 0.0015\n",
            "Epoch 378/500\n",
            "158/158 [==============================] - 0s 3ms/step - loss: 3.7768 - val_loss: 0.0012\n",
            "Epoch 379/500\n",
            "158/158 [==============================] - 0s 3ms/step - loss: 3.7754 - val_loss: 0.0013\n",
            "Epoch 380/500\n",
            "158/158 [==============================] - 0s 3ms/step - loss: 3.7739 - val_loss: 0.0014\n",
            "Epoch 381/500\n",
            "158/158 [==============================] - 0s 3ms/step - loss: 3.7726 - val_loss: 0.0016\n",
            "Epoch 382/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.7712 - val_loss: 0.0015\n",
            "Epoch 383/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.7699 - val_loss: 0.0013\n",
            "Epoch 384/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.7685 - val_loss: 0.0018\n",
            "Epoch 385/500\n",
            "158/158 [==============================] - 1s 3ms/step - loss: 3.7671 - val_loss: 0.0018\n",
            "Epoch 386/500\n",
            "158/158 [==============================] - 0s 3ms/step - loss: 3.7657 - val_loss: 0.0017\n",
            "Epoch 387/500\n",
            "158/158 [==============================] - 1s 4ms/step - loss: 3.7642 - val_loss: 0.0018\n",
            "Epoch 388/500\n",
            "158/158 [==============================] - 1s 5ms/step - loss: 3.7629 - val_loss: 0.0018\n",
            "Epoch 389/500\n",
            "158/158 [==============================] - 0s 3ms/step - loss: 3.7615 - val_loss: 0.0024\n",
            "Epoch 390/500\n",
            "158/158 [==============================] - 1s 4ms/step - loss: 3.7604 - val_loss: 0.0024\n",
            "Epoch 391/500\n",
            "158/158 [==============================] - 1s 6ms/step - loss: 3.7587 - val_loss: 0.0019\n",
            "Epoch 392/500\n",
            "158/158 [==============================] - 1s 3ms/step - loss: 3.7573 - val_loss: 0.0018\n",
            "Epoch 393/500\n",
            "158/158 [==============================] - 1s 3ms/step - loss: 3.7560 - val_loss: 0.0020\n",
            "Epoch 394/500\n",
            "158/158 [==============================] - 0s 3ms/step - loss: 3.7549 - val_loss: 0.0018\n",
            "Epoch 395/500\n",
            "158/158 [==============================] - 1s 6ms/step - loss: 3.7532 - val_loss: 0.0019\n",
            "Epoch 396/500\n",
            "158/158 [==============================] - 1s 4ms/step - loss: 3.7520 - val_loss: 0.0017\n",
            "Epoch 397/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.7506 - val_loss: 0.0019\n",
            "Epoch 398/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.7492 - val_loss: 0.0019\n",
            "Epoch 399/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.7479 - val_loss: 0.0020\n",
            "Epoch 400/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.7467 - val_loss: 0.0030\n",
            "Epoch 401/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.7451 - val_loss: 0.0020\n",
            "Epoch 402/500\n",
            "158/158 [==============================] - 0s 3ms/step - loss: 3.7439 - val_loss: 0.0019\n",
            "Epoch 403/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.7424 - val_loss: 0.0018\n",
            "Epoch 404/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.7411 - val_loss: 0.0019\n",
            "Epoch 405/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.7397 - val_loss: 0.0021\n",
            "Epoch 406/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.7387 - val_loss: 0.0020\n",
            "Epoch 407/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.7371 - val_loss: 0.0018\n",
            "Epoch 408/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.7359 - val_loss: 0.0020\n",
            "Epoch 409/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.7348 - val_loss: 0.0018\n",
            "Epoch 410/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.7330 - val_loss: 0.0021\n",
            "Epoch 411/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.7317 - val_loss: 0.0020\n",
            "Epoch 412/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.7302 - val_loss: 0.0019\n",
            "Epoch 413/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.7289 - val_loss: 0.0019\n",
            "Epoch 414/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.7278 - val_loss: 0.0018\n",
            "Epoch 415/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.7262 - val_loss: 0.0020\n",
            "Epoch 416/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.7248 - val_loss: 0.0020\n",
            "Epoch 417/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.7235 - val_loss: 0.0025\n",
            "Epoch 418/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.7224 - val_loss: 0.0020\n",
            "Epoch 419/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.7209 - val_loss: 0.0020\n",
            "Epoch 420/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.7196 - val_loss: 0.0018\n",
            "Epoch 421/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.7183 - val_loss: 0.0016\n",
            "Epoch 422/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.7169 - val_loss: 0.0016\n",
            "Epoch 423/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.7157 - val_loss: 0.0018\n",
            "Epoch 424/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.7141 - val_loss: 0.0014\n",
            "Epoch 425/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 4.0489 - val_loss: 9.7758e-04\n",
            "Epoch 426/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 4.0469 - val_loss: 0.0012\n",
            "Epoch 427/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 4.0447 - val_loss: 0.0013\n",
            "Epoch 428/500\n",
            "158/158 [==============================] - 1s 5ms/step - loss: 4.0418 - val_loss: 0.0014\n",
            "Epoch 429/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 4.0402 - val_loss: 0.0015\n",
            "Epoch 430/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 4.0387 - val_loss: 0.0017\n",
            "Epoch 431/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 4.0371 - val_loss: 0.0015\n",
            "Epoch 432/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 4.0356 - val_loss: 0.0016\n",
            "Epoch 433/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 4.0340 - val_loss: 0.0014\n",
            "Epoch 434/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 4.0325 - val_loss: 0.0015\n",
            "Epoch 435/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 4.0307 - val_loss: 0.0017\n",
            "Epoch 436/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 4.0294 - val_loss: 0.0016\n",
            "Epoch 437/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 4.0277 - val_loss: 0.0017\n",
            "Epoch 438/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 4.0260 - val_loss: 0.0019\n",
            "Epoch 439/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 4.0245 - val_loss: 0.0016\n",
            "Epoch 440/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 4.0228 - val_loss: 0.0016\n",
            "Epoch 441/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 4.0216 - val_loss: 0.0017\n",
            "Epoch 442/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.9022 - val_loss: 0.0015\n",
            "Epoch 443/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.9005 - val_loss: 0.0015\n",
            "Epoch 444/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.8992 - val_loss: 0.0014\n",
            "Epoch 445/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.8977 - val_loss: 0.0014\n",
            "Epoch 446/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.8962 - val_loss: 0.0014\n",
            "Epoch 447/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.8947 - val_loss: 0.0022\n",
            "Epoch 448/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.8935 - val_loss: 0.0013\n",
            "Epoch 449/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.8916 - val_loss: 0.0013\n",
            "Epoch 450/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.8902 - val_loss: 0.0013\n",
            "Epoch 451/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.8887 - val_loss: 0.0025\n",
            "Epoch 452/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.8873 - val_loss: 0.0012\n",
            "Epoch 453/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.8856 - val_loss: 0.0022\n",
            "Epoch 454/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.8842 - val_loss: 0.0012\n",
            "Epoch 455/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.8826 - val_loss: 0.0018\n",
            "Epoch 456/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.8812 - val_loss: 0.0013\n",
            "Epoch 457/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.8797 - val_loss: 0.0013\n",
            "Epoch 458/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.8782 - val_loss: 0.0013\n",
            "Epoch 459/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.8768 - val_loss: 0.0011\n",
            "Epoch 460/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.8751 - val_loss: 0.0032\n",
            "Epoch 461/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.8738 - val_loss: 0.0011\n",
            "Epoch 462/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.8722 - val_loss: 0.0011\n",
            "Epoch 463/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.8708 - val_loss: 0.0010\n",
            "Epoch 464/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.8695 - val_loss: 0.0010\n",
            "Epoch 465/500\n",
            "158/158 [==============================] - 1s 3ms/step - loss: 3.8679 - val_loss: 9.5843e-04\n",
            "Epoch 466/500\n",
            "158/158 [==============================] - 1s 3ms/step - loss: 3.8662 - val_loss: 9.4196e-04\n",
            "Epoch 467/500\n",
            "158/158 [==============================] - 1s 4ms/step - loss: 3.8649 - val_loss: 9.3805e-04\n",
            "Epoch 468/500\n",
            "158/158 [==============================] - 1s 3ms/step - loss: 3.8632 - val_loss: 9.7461e-04\n",
            "Epoch 469/500\n",
            "158/158 [==============================] - 1s 5ms/step - loss: 3.8619 - val_loss: 9.4080e-04\n",
            "Epoch 470/500\n",
            "158/158 [==============================] - 1s 3ms/step - loss: 3.8602 - val_loss: 9.4482e-04\n",
            "Epoch 471/500\n",
            "158/158 [==============================] - 1s 4ms/step - loss: 3.8588 - val_loss: 9.1443e-04\n",
            "Epoch 472/500\n",
            "158/158 [==============================] - 1s 4ms/step - loss: 3.8572 - val_loss: 0.0016\n",
            "Epoch 473/500\n",
            "158/158 [==============================] - 1s 5ms/step - loss: 3.8557 - val_loss: 8.7660e-04\n",
            "Epoch 474/500\n",
            "158/158 [==============================] - 0s 3ms/step - loss: 3.8542 - val_loss: 0.0037\n",
            "Epoch 475/500\n",
            "158/158 [==============================] - 1s 3ms/step - loss: 3.8528 - val_loss: 0.0011\n",
            "Epoch 476/500\n",
            "158/158 [==============================] - 1s 4ms/step - loss: 3.8512 - val_loss: 8.7669e-04\n",
            "Epoch 477/500\n",
            "158/158 [==============================] - 0s 3ms/step - loss: 3.8496 - val_loss: 9.0358e-04\n",
            "Epoch 478/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.8484 - val_loss: 0.0010\n",
            "Epoch 479/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.8468 - val_loss: 0.0011\n",
            "Epoch 480/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.8453 - val_loss: 8.6360e-04\n",
            "Epoch 481/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.8438 - val_loss: 8.9861e-04\n",
            "Epoch 482/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.8428 - val_loss: 8.7345e-04\n",
            "Epoch 483/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.8409 - val_loss: 8.6701e-04\n",
            "Epoch 484/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.8398 - val_loss: 9.1744e-04\n",
            "Epoch 485/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 3.8380 - val_loss: 9.4254e-04\n",
            "Epoch 486/500\n",
            "158/158 [==============================] - 0s 3ms/step - loss: 3.8371 - val_loss: 9.4121e-04\n",
            "Epoch 487/500\n",
            "158/158 [==============================] - 0s 3ms/step - loss: 3.8355 - val_loss: 0.0011\n",
            "Epoch 488/500\n",
            "158/158 [==============================] - 0s 3ms/step - loss: 3.8336 - val_loss: 9.2801e-04\n",
            "Epoch 489/500\n",
            "158/158 [==============================] - 0s 3ms/step - loss: 3.8322 - val_loss: 9.1093e-04\n",
            "Epoch 490/500\n",
            "158/158 [==============================] - 0s 3ms/step - loss: 3.8553 - val_loss: 0.0011\n",
            "Epoch 491/500\n",
            "158/158 [==============================] - 0s 3ms/step - loss: 4.0839 - val_loss: 0.0052\n",
            "Epoch 492/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 4.0823 - val_loss: 0.0010\n",
            "Epoch 493/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 4.0800 - val_loss: 0.0010\n",
            "Epoch 494/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 4.0781 - val_loss: 0.0010\n",
            "Epoch 495/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 4.0767 - val_loss: 0.0013\n",
            "Epoch 496/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 4.0745 - val_loss: 0.0012\n",
            "Epoch 497/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 4.0726 - val_loss: 0.0013\n",
            "Epoch 498/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 4.0708 - val_loss: 0.0013\n",
            "Epoch 499/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 4.0695 - val_loss: 0.0019\n",
            "Epoch 500/500\n",
            "158/158 [==============================] - 0s 2ms/step - loss: 4.0674 - val_loss: 0.0022\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prediction based on limited number of fourier components and predicted difference"
      ],
      "metadata": {
        "id": "U7AadO2Jhfo6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_input = np.array(errfour).reshape(len(errfour), dim_targetdata)\n",
        "test_output = np.array(modelauto.predict(test_input, verbose=0)) # = echt - fourier"
      ],
      "metadata": {
        "id": "uuhXocmShus5"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "uiteindelijkeschatting = test_output + predictie"
      ],
      "metadata": {
        "id": "sm_beURKhgRq"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prediction of the parameters"
      ],
      "metadata": {
        "id": "1t9u9HWOhymf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fourtopred = np.concatenate([predfour[:,0:5,0],predfour[:,1:4,1]],axis = 1) # 1:4 because first one is always 0\n",
        "#obtain the hidden vectors\n",
        "Xen = np.array(errfour).reshape(len(errfour), dim_targetdata)\n",
        "layer_output = modelauto.get_layer('dense_2').output #dense can need an extra number, depending on how much the algorithm is performed\n",
        "intermediate_model=tf.keras.models.Model(inputs=params_input,outputs=layer_output)\n",
        "tussen=intermediate_model.predict(Xen) # = hidden vectors!\n",
        "tevoorspellen = np.concatenate([fourtopred,tussen],axis = 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QkWZoZnch0lW",
        "outputId": "3065e904-7fc9-40c7-e552-aab285df6139"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "230/230 [==============================] - 0s 948us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "indexen = range(len(features))\n",
        "X_train_four, X_test_four, indexen_train_four, indexen_test_four = train_test_split(grids, indexen, train_size=train_size, test_size=test_size, random_state=333)\n",
        "training_y_four = fourtopred[indexen_train_four]\n",
        "y_test_four = fourtopred[indexen_test_four]\n",
        "\n",
        "X = np.array(X_train_four).reshape(len(X_train_four), dimgrids,dimgrids,1)\n",
        "\n",
        "Y = np.array(training_y_four).reshape(len(training_y_four), 8)\n",
        "#using a standardscaler here is advised"
      ],
      "metadata": {
        "id": "J9pXpZTLii0q"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make and train the neural network"
      ],
      "metadata": {
        "id": "HuD7mQsWjRNl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "modelsfour = Sequential()\n",
        "modelsfour.add(Conv2D(num_feature_maps,kernel_size=kernel_size,activation='relu',input_shape=(dimgrids, dimgrids, 1)))\n",
        "modelsfour.add(MaxPooling2D(2, 2))\n",
        "modelsfour.add(Flatten())\n",
        "modelsfour.add(Dense(dense_layer_size, activation='relu'))\n",
        "modelsfour.add(Dropout(0.2))\n",
        "modelsfour.add(Dense(8))\n",
        "modelsfour.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"accuracy\"])\n",
        "modelsfour.summary()\n",
        "hist = modelsfour.fit(X,Y,epochs=100,validation_split=0.2,batch_size=32) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UayXGd4vMc86",
        "outputId": "3aadbd19-6ec4-4dcc-f958-d09e4183fec1"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_1 (Conv2D)           (None, 61, 61, 9)         333       \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 30, 30, 9)        0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 8100)              0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 2048)              16590848  \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 2048)              0         \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 8)                 16392     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 16,607,573\n",
            "Trainable params: 16,607,573\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "158/158 [==============================] - 36s 223ms/step - loss: 1429.9437 - accuracy: 0.4972 - val_loss: 15.1202 - val_accuracy: 0.5317\n",
            "Epoch 2/100\n",
            "158/158 [==============================] - 35s 221ms/step - loss: 1423.8682 - accuracy: 0.5192 - val_loss: 12.9067 - val_accuracy: 0.5492\n",
            "Epoch 3/100\n",
            "158/158 [==============================] - 35s 221ms/step - loss: 1423.0469 - accuracy: 0.5419 - val_loss: 13.6560 - val_accuracy: 0.5770\n",
            "Epoch 4/100\n",
            "158/158 [==============================] - 34s 217ms/step - loss: 1410.5750 - accuracy: 0.5274 - val_loss: 16.6035 - val_accuracy: 0.5532\n",
            "Epoch 5/100\n",
            "158/158 [==============================] - 35s 221ms/step - loss: 1385.1179 - accuracy: 0.4859 - val_loss: 17.7673 - val_accuracy: 0.3516\n",
            "Epoch 6/100\n",
            "158/158 [==============================] - 36s 229ms/step - loss: 1364.5657 - accuracy: 0.5071 - val_loss: 41.6111 - val_accuracy: 0.4881\n",
            "Epoch 7/100\n",
            "158/158 [==============================] - 37s 232ms/step - loss: 1291.3475 - accuracy: 0.5266 - val_loss: 52.2777 - val_accuracy: 0.5817\n",
            "Epoch 8/100\n",
            "158/158 [==============================] - 34s 213ms/step - loss: 1244.7395 - accuracy: 0.4925 - val_loss: 109.3051 - val_accuracy: 0.4579\n",
            "Epoch 9/100\n",
            "158/158 [==============================] - 34s 213ms/step - loss: 1076.9976 - accuracy: 0.5159 - val_loss: 17.7161 - val_accuracy: 0.4762\n",
            "Epoch 10/100\n",
            "158/158 [==============================] - 33s 212ms/step - loss: 1171.9170 - accuracy: 0.4879 - val_loss: 51.5285 - val_accuracy: 0.4492\n",
            "Epoch 11/100\n",
            "158/158 [==============================] - 33s 209ms/step - loss: 941.0679 - accuracy: 0.4819 - val_loss: 201.6814 - val_accuracy: 0.4794\n",
            "Epoch 12/100\n",
            "158/158 [==============================] - 34s 212ms/step - loss: 831.2040 - accuracy: 0.4950 - val_loss: 227.8961 - val_accuracy: 0.4944\n",
            "Epoch 13/100\n",
            "158/158 [==============================] - 34s 214ms/step - loss: 732.0646 - accuracy: 0.5099 - val_loss: 40.8893 - val_accuracy: 0.5690\n",
            "Epoch 14/100\n",
            "158/158 [==============================] - 33s 209ms/step - loss: 750.0138 - accuracy: 0.5099 - val_loss: 80.4070 - val_accuracy: 0.5286\n",
            "Epoch 15/100\n",
            "158/158 [==============================] - 34s 215ms/step - loss: 471.6276 - accuracy: 0.5105 - val_loss: 176.1699 - val_accuracy: 0.5714\n",
            "Epoch 16/100\n",
            "158/158 [==============================] - 34s 217ms/step - loss: 464.0199 - accuracy: 0.5111 - val_loss: 34.7859 - val_accuracy: 0.5421\n",
            "Epoch 17/100\n",
            "158/158 [==============================] - 34s 214ms/step - loss: 335.9171 - accuracy: 0.5032 - val_loss: 205.6774 - val_accuracy: 0.5246\n",
            "Epoch 18/100\n",
            "158/158 [==============================] - 33s 207ms/step - loss: 195.8960 - accuracy: 0.4984 - val_loss: 27.1159 - val_accuracy: 0.5365\n",
            "Epoch 19/100\n",
            "158/158 [==============================] - 34s 213ms/step - loss: 256.9315 - accuracy: 0.4996 - val_loss: 36.4058 - val_accuracy: 0.5524\n",
            "Epoch 20/100\n",
            "158/158 [==============================] - 34s 213ms/step - loss: 112.7944 - accuracy: 0.5062 - val_loss: 95.4357 - val_accuracy: 0.5294\n",
            "Epoch 21/100\n",
            "158/158 [==============================] - 33s 207ms/step - loss: 67.1195 - accuracy: 0.4972 - val_loss: 23.3968 - val_accuracy: 0.5524\n",
            "Epoch 22/100\n",
            "158/158 [==============================] - 34s 214ms/step - loss: 85.1573 - accuracy: 0.5042 - val_loss: 19.0456 - val_accuracy: 0.5984\n",
            "Epoch 23/100\n",
            "158/158 [==============================] - 34s 213ms/step - loss: 41.8260 - accuracy: 0.5091 - val_loss: 43.1927 - val_accuracy: 0.5595\n",
            "Epoch 24/100\n",
            "158/158 [==============================] - 33s 210ms/step - loss: 33.8797 - accuracy: 0.5060 - val_loss: 16.9595 - val_accuracy: 0.5595\n",
            "Epoch 25/100\n",
            "158/158 [==============================] - 34s 213ms/step - loss: 19.4081 - accuracy: 0.5054 - val_loss: 15.5480 - val_accuracy: 0.5849\n",
            "Epoch 26/100\n",
            "158/158 [==============================] - 34s 212ms/step - loss: 14.2361 - accuracy: 0.4960 - val_loss: 16.0054 - val_accuracy: 0.5286\n",
            "Epoch 27/100\n",
            "158/158 [==============================] - 33s 210ms/step - loss: 11.8067 - accuracy: 0.5062 - val_loss: 14.7612 - val_accuracy: 0.5897\n",
            "Epoch 28/100\n",
            "158/158 [==============================] - 33s 209ms/step - loss: 13.0805 - accuracy: 0.5123 - val_loss: 17.2269 - val_accuracy: 0.5563\n",
            "Epoch 29/100\n",
            "158/158 [==============================] - 34s 212ms/step - loss: 23.3133 - accuracy: 0.5058 - val_loss: 13.3278 - val_accuracy: 0.5714\n",
            "Epoch 30/100\n",
            "158/158 [==============================] - 33s 212ms/step - loss: 173.1713 - accuracy: 0.5149 - val_loss: 14.3929 - val_accuracy: 0.5587\n",
            "Epoch 31/100\n",
            "158/158 [==============================] - 33s 208ms/step - loss: 13.0049 - accuracy: 0.4978 - val_loss: 14.1142 - val_accuracy: 0.4881\n",
            "Epoch 32/100\n",
            "158/158 [==============================] - 34s 213ms/step - loss: 9.8735 - accuracy: 0.5133 - val_loss: 13.0938 - val_accuracy: 0.5627\n",
            "Epoch 33/100\n",
            "158/158 [==============================] - 34s 214ms/step - loss: 19.4311 - accuracy: 0.5065 - val_loss: 18.7167 - val_accuracy: 0.5468\n",
            "Epoch 34/100\n",
            "158/158 [==============================] - 34s 214ms/step - loss: 54.9526 - accuracy: 0.5018 - val_loss: 13.9425 - val_accuracy: 0.5325\n",
            "Epoch 35/100\n",
            "158/158 [==============================] - 34s 212ms/step - loss: 336.9214 - accuracy: 0.4964 - val_loss: 16.4369 - val_accuracy: 0.5341\n",
            "Epoch 36/100\n",
            "158/158 [==============================] - 33s 212ms/step - loss: 10.0368 - accuracy: 0.5026 - val_loss: 13.7617 - val_accuracy: 0.5476\n",
            "Epoch 37/100\n",
            "158/158 [==============================] - 34s 212ms/step - loss: 9.8392 - accuracy: 0.5006 - val_loss: 13.8282 - val_accuracy: 0.5635\n",
            "Epoch 38/100\n",
            "158/158 [==============================] - 33s 208ms/step - loss: 10.3451 - accuracy: 0.5069 - val_loss: 13.7424 - val_accuracy: 0.5452\n",
            "Epoch 39/100\n",
            "158/158 [==============================] - 34s 213ms/step - loss: 9.5787 - accuracy: 0.5067 - val_loss: 13.9083 - val_accuracy: 0.5698\n",
            "Epoch 40/100\n",
            "158/158 [==============================] - 33s 212ms/step - loss: 10.6859 - accuracy: 0.5014 - val_loss: 13.5587 - val_accuracy: 0.5659\n",
            "Epoch 41/100\n",
            "158/158 [==============================] - 33s 207ms/step - loss: 8.7658 - accuracy: 0.5083 - val_loss: 13.6295 - val_accuracy: 0.5762\n",
            "Epoch 42/100\n",
            "158/158 [==============================] - 34s 213ms/step - loss: 13.8307 - accuracy: 0.5056 - val_loss: 14.2868 - val_accuracy: 0.5357\n",
            "Epoch 43/100\n",
            "158/158 [==============================] - 33s 212ms/step - loss: 17.6136 - accuracy: 0.5038 - val_loss: 13.1635 - val_accuracy: 0.5587\n",
            "Epoch 44/100\n",
            "158/158 [==============================] - 33s 208ms/step - loss: 92.0574 - accuracy: 0.5032 - val_loss: 21.4953 - val_accuracy: 0.5294\n",
            "Epoch 45/100\n",
            "158/158 [==============================] - 34s 213ms/step - loss: 82.0967 - accuracy: 0.4933 - val_loss: 12.7485 - val_accuracy: 0.5381\n",
            "Epoch 46/100\n",
            "158/158 [==============================] - 34s 213ms/step - loss: 283.1672 - accuracy: 0.4996 - val_loss: 17.5956 - val_accuracy: 0.5571\n",
            "Epoch 47/100\n",
            "158/158 [==============================] - 33s 207ms/step - loss: 35.6633 - accuracy: 0.5012 - val_loss: 12.8757 - val_accuracy: 0.5429\n",
            "Epoch 48/100\n",
            "158/158 [==============================] - 34s 212ms/step - loss: 72.3899 - accuracy: 0.5091 - val_loss: 21.4121 - val_accuracy: 0.5619\n",
            "Epoch 49/100\n",
            "158/158 [==============================] - 34s 212ms/step - loss: 68.4420 - accuracy: 0.5083 - val_loss: 12.6255 - val_accuracy: 0.5611\n",
            "Epoch 50/100\n",
            "158/158 [==============================] - 33s 212ms/step - loss: 98.2814 - accuracy: 0.5115 - val_loss: 19.4206 - val_accuracy: 0.5762\n",
            "Epoch 51/100\n",
            "158/158 [==============================] - 33s 207ms/step - loss: 46.3301 - accuracy: 0.5171 - val_loss: 12.8411 - val_accuracy: 0.5706\n",
            "Epoch 52/100\n",
            "158/158 [==============================] - 34s 213ms/step - loss: 66.3233 - accuracy: 0.5125 - val_loss: 17.4998 - val_accuracy: 0.5548\n",
            "Epoch 53/100\n",
            "158/158 [==============================] - 34s 212ms/step - loss: 35.1327 - accuracy: 0.4978 - val_loss: 13.1541 - val_accuracy: 0.5159\n",
            "Epoch 54/100\n",
            "158/158 [==============================] - 33s 209ms/step - loss: 38.1577 - accuracy: 0.4944 - val_loss: 15.3711 - val_accuracy: 0.5389\n",
            "Epoch 55/100\n",
            "158/158 [==============================] - 33s 212ms/step - loss: 28.7470 - accuracy: 0.5071 - val_loss: 12.5038 - val_accuracy: 0.5746\n",
            "Epoch 56/100\n",
            "158/158 [==============================] - 34s 213ms/step - loss: 43.0318 - accuracy: 0.5067 - val_loss: 16.7359 - val_accuracy: 0.5103\n",
            "Epoch 57/100\n",
            "158/158 [==============================] - 33s 208ms/step - loss: 72.2116 - accuracy: 0.4990 - val_loss: 12.5927 - val_accuracy: 0.5246\n",
            "Epoch 58/100\n",
            "158/158 [==============================] - 34s 213ms/step - loss: 167.5482 - accuracy: 0.5137 - val_loss: 29.8505 - val_accuracy: 0.5698\n",
            "Epoch 59/100\n",
            "158/158 [==============================] - 33s 211ms/step - loss: 43.3110 - accuracy: 0.5050 - val_loss: 12.7310 - val_accuracy: 0.5524\n",
            "Epoch 60/100\n",
            "158/158 [==============================] - 33s 208ms/step - loss: 97.3650 - accuracy: 0.5054 - val_loss: 40.2100 - val_accuracy: 0.5103\n",
            "Epoch 61/100\n",
            "158/158 [==============================] - 33s 211ms/step - loss: 122.6423 - accuracy: 0.5004 - val_loss: 12.4410 - val_accuracy: 0.5413\n",
            "Epoch 62/100\n",
            "158/158 [==============================] - 34s 217ms/step - loss: 107.4768 - accuracy: 0.5119 - val_loss: 13.3466 - val_accuracy: 0.5683\n",
            "Epoch 63/100\n",
            "158/158 [==============================] - 34s 215ms/step - loss: 8.3975 - accuracy: 0.5129 - val_loss: 13.1438 - val_accuracy: 0.5127\n",
            "Epoch 64/100\n",
            "158/158 [==============================] - 33s 208ms/step - loss: 9.6865 - accuracy: 0.4909 - val_loss: 14.1402 - val_accuracy: 0.5040\n",
            "Epoch 65/100\n",
            "158/158 [==============================] - 34s 213ms/step - loss: 8.7447 - accuracy: 0.5081 - val_loss: 12.8566 - val_accuracy: 0.5349\n",
            "Epoch 66/100\n",
            "158/158 [==============================] - 34s 214ms/step - loss: 9.0891 - accuracy: 0.5155 - val_loss: 12.9244 - val_accuracy: 0.5341\n",
            "Epoch 67/100\n",
            "158/158 [==============================] - 33s 210ms/step - loss: 27.5259 - accuracy: 0.5137 - val_loss: 13.1609 - val_accuracy: 0.5786\n",
            "Epoch 68/100\n",
            "158/158 [==============================] - 34s 213ms/step - loss: 29.2216 - accuracy: 0.5044 - val_loss: 12.5357 - val_accuracy: 0.6476\n",
            "Epoch 69/100\n",
            "158/158 [==============================] - 34s 214ms/step - loss: 45.5587 - accuracy: 0.5153 - val_loss: 13.2402 - val_accuracy: 0.5706\n",
            "Epoch 70/100\n",
            "158/158 [==============================] - 33s 210ms/step - loss: 72.7540 - accuracy: 0.5032 - val_loss: 12.3855 - val_accuracy: 0.5651\n",
            "Epoch 71/100\n",
            "158/158 [==============================] - 34s 215ms/step - loss: 179.2237 - accuracy: 0.5129 - val_loss: 21.6271 - val_accuracy: 0.5159\n",
            "Epoch 72/100\n",
            "158/158 [==============================] - 34s 215ms/step - loss: 52.2556 - accuracy: 0.5091 - val_loss: 12.5763 - val_accuracy: 0.5421\n",
            "Epoch 73/100\n",
            "158/158 [==============================] - 34s 214ms/step - loss: 62.2042 - accuracy: 0.5008 - val_loss: 14.1250 - val_accuracy: 0.5524\n",
            "Epoch 74/100\n",
            "158/158 [==============================] - 33s 209ms/step - loss: 38.6134 - accuracy: 0.5065 - val_loss: 12.2727 - val_accuracy: 0.5992\n",
            "Epoch 75/100\n",
            "158/158 [==============================] - 33s 212ms/step - loss: 27.8243 - accuracy: 0.5149 - val_loss: 13.1114 - val_accuracy: 0.5706\n",
            "Epoch 76/100\n",
            "158/158 [==============================] - 33s 211ms/step - loss: 36.2690 - accuracy: 0.5133 - val_loss: 13.3562 - val_accuracy: 0.5294\n",
            "Epoch 77/100\n",
            "158/158 [==============================] - 32s 203ms/step - loss: 67.8964 - accuracy: 0.5139 - val_loss: 13.9217 - val_accuracy: 0.5238\n",
            "Epoch 78/100\n",
            "158/158 [==============================] - 33s 208ms/step - loss: 52.2580 - accuracy: 0.5190 - val_loss: 12.7262 - val_accuracy: 0.5373\n",
            "Epoch 79/100\n",
            "158/158 [==============================] - 33s 208ms/step - loss: 120.1934 - accuracy: 0.5095 - val_loss: 12.8842 - val_accuracy: 0.5468\n",
            "Epoch 80/100\n",
            "158/158 [==============================] - 32s 203ms/step - loss: 39.0723 - accuracy: 0.5040 - val_loss: 12.3832 - val_accuracy: 0.5468\n",
            "Epoch 81/100\n",
            "158/158 [==============================] - 33s 208ms/step - loss: 65.3684 - accuracy: 0.5063 - val_loss: 15.9494 - val_accuracy: 0.5143\n",
            "Epoch 82/100\n",
            "158/158 [==============================] - 33s 208ms/step - loss: 58.0944 - accuracy: 0.4986 - val_loss: 12.2651 - val_accuracy: 0.5516\n",
            "Epoch 83/100\n",
            "158/158 [==============================] - 32s 203ms/step - loss: 49.8025 - accuracy: 0.4843 - val_loss: 15.7348 - val_accuracy: 0.5357\n",
            "Epoch 84/100\n",
            "158/158 [==============================] - 33s 207ms/step - loss: 31.7677 - accuracy: 0.5006 - val_loss: 11.9129 - val_accuracy: 0.5317\n",
            "Epoch 85/100\n",
            "158/158 [==============================] - 33s 209ms/step - loss: 51.8130 - accuracy: 0.5115 - val_loss: 14.8952 - val_accuracy: 0.5484\n",
            "Epoch 86/100\n",
            "158/158 [==============================] - 32s 202ms/step - loss: 46.2287 - accuracy: 0.5121 - val_loss: 11.9824 - val_accuracy: 0.5103\n",
            "Epoch 87/100\n",
            "158/158 [==============================] - 33s 208ms/step - loss: 58.6334 - accuracy: 0.4970 - val_loss: 12.8862 - val_accuracy: 0.5183\n",
            "Epoch 88/100\n",
            "158/158 [==============================] - 33s 207ms/step - loss: 16.0552 - accuracy: 0.5050 - val_loss: 12.4461 - val_accuracy: 0.5135\n",
            "Epoch 89/100\n",
            "158/158 [==============================] - 32s 205ms/step - loss: 15.5073 - accuracy: 0.5099 - val_loss: 12.9466 - val_accuracy: 0.5040\n",
            "Epoch 90/100\n",
            "158/158 [==============================] - 33s 209ms/step - loss: 24.3401 - accuracy: 0.5143 - val_loss: 12.2682 - val_accuracy: 0.5984\n",
            "Epoch 91/100\n",
            "158/158 [==============================] - 33s 210ms/step - loss: 82.1531 - accuracy: 0.4927 - val_loss: 16.8006 - val_accuracy: 0.5143\n",
            "Epoch 92/100\n",
            "158/158 [==============================] - 32s 204ms/step - loss: 66.4567 - accuracy: 0.5105 - val_loss: 12.9875 - val_accuracy: 0.5317\n",
            "Epoch 93/100\n",
            "158/158 [==============================] - 34s 214ms/step - loss: 98.9796 - accuracy: 0.4976 - val_loss: 30.4102 - val_accuracy: 0.5270\n",
            "Epoch 94/100\n",
            "158/158 [==============================] - 34s 214ms/step - loss: 33.7240 - accuracy: 0.4978 - val_loss: 12.4911 - val_accuracy: 0.5675\n",
            "Epoch 95/100\n",
            "158/158 [==============================] - 33s 209ms/step - loss: 29.0385 - accuracy: 0.5034 - val_loss: 13.0726 - val_accuracy: 0.5683\n",
            "Epoch 96/100\n",
            "158/158 [==============================] - 34s 215ms/step - loss: 34.6242 - accuracy: 0.5056 - val_loss: 12.5135 - val_accuracy: 0.5794\n",
            "Epoch 97/100\n",
            "158/158 [==============================] - 34s 215ms/step - loss: 68.2656 - accuracy: 0.5024 - val_loss: 17.2201 - val_accuracy: 0.6119\n",
            "Epoch 98/100\n",
            "158/158 [==============================] - 32s 206ms/step - loss: 71.3593 - accuracy: 0.5038 - val_loss: 12.1303 - val_accuracy: 0.5238\n",
            "Epoch 99/100\n",
            "158/158 [==============================] - 33s 210ms/step - loss: 109.5933 - accuracy: 0.4917 - val_loss: 13.3868 - val_accuracy: 0.5357\n",
            "Epoch 100/100\n",
            "158/158 [==============================] - 33s 211ms/step - loss: 30.9702 - accuracy: 0.5075 - val_loss: 11.7204 - val_accuracy: 0.5452\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_input = np.array(grids).reshape(len(grids), dimgrids,dimgrids,1)\n",
        "test_output = np.array(modelsfour.predict(test_input, verbose=0))\n",
        "fourpredictienn = np.vstack(np.transpose(np.array((test_output[:,0],test_output[:,1],test_output[:,2],test_output[:,3],test_output[:,4],np.zeros(len(test_output)),test_output[:,5],test_output[:,6],test_output[:,7]))))"
      ],
      "metadata": {
        "id": "_pAk4Vpqjnbx"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Predict the hidden state"
      ],
      "metadata": {
        "id": "l3kTVUZJjy6h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "indexen = range(len(features))\n",
        "feathid = []\n",
        "#add the predicted fouriercomponents to the input of hiddenstate prediction\n",
        "for ii in range(len(features)):\n",
        "  feathid.append(np.concatenate([features[ii], fourpredictienn[ii]]))\n",
        "\n",
        "\n",
        "X_train_hid, X_test_hid, indexen_train_hid, indexen_test_hid = train_test_split(grids, indexen, train_size= train_size, test_size=test_size, random_state=333)\n",
        "training_y_hid = tussen[indexen_train_hid]\n",
        "y_test_hid = tussen[indexen_test_hid]\n",
        "X2_train = fourpredictienn[indexen_train_hid]\n",
        "X2_test = fourpredictienn[indexen_test_hid]\n",
        "\n",
        "X1 = np.array(X_train_hid).reshape(len(X_train_hid), dimgrids,dimgrids,1)\n",
        "X2 = np.array(X2_train).reshape(len(X2_train),9)\n",
        "\n",
        "Y = np.array(training_y_hid).reshape(len(training_y_hid), 5)\n",
        "#again it's better to standardscale"
      ],
      "metadata": {
        "id": "pdB1fiBmNocx"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inp1 = keras.Input(shape=(dimgrids,dimgrids,1))\n",
        "inp2 = keras.Input(shape=(9))\n",
        "convlayer = Conv2D(num_feature_maps,kernel_size=kernel_size,activation='relu',input_shape=(dimgrids, dimgrids, 1))(inp1)\n",
        "maxpool = MaxPooling2D(2,2)(convlayer)\n",
        "flatlay = Flatten()(maxpool)\n",
        "denselay = Dense(dense_layer_size, activation='relu')(flatlay)\n",
        "droplay = Dropout(0.2)(denselay)\n",
        "conclay = layers.concatenate([droplay,inp2])\n",
        "denselas = (Dense(5))(conclay)\n",
        "modelhid = keras.Model(inputs=(inp1,inp2),outputs=denselas)\n",
        "modelhid.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"accuracy\"])\n",
        "modelhid.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6RHTy0UxNoR5",
        "outputId": "456fb740-d437-4842-ae4e-6f1605909d62"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 66, 66, 1)]  0           []                               \n",
            "                                                                                                  \n",
            " conv2d_2 (Conv2D)              (None, 61, 61, 9)    333         ['input_3[0][0]']                \n",
            "                                                                                                  \n",
            " max_pooling2d_3 (MaxPooling2D)  (None, 30, 30, 9)   0           ['conv2d_2[0][0]']               \n",
            "                                                                                                  \n",
            " flatten_2 (Flatten)            (None, 8100)         0           ['max_pooling2d_3[0][0]']        \n",
            "                                                                                                  \n",
            " dense_8 (Dense)                (None, 2048)         16590848    ['flatten_2[0][0]']              \n",
            "                                                                                                  \n",
            " dropout_2 (Dropout)            (None, 2048)         0           ['dense_8[0][0]']                \n",
            "                                                                                                  \n",
            " input_4 (InputLayer)           [(None, 9)]          0           []                               \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 2057)         0           ['dropout_2[0][0]',              \n",
            "                                                                  'input_4[0][0]']                \n",
            "                                                                                                  \n",
            " dense_9 (Dense)                (None, 5)            10290       ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 16,601,471\n",
            "Trainable params: 16,601,471\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hist = modelhid.fit((X1,X2),Y,epochs=100,validation_split=0.2,batch_size=32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EQ-qSkAzOdHr",
        "outputId": "db741906-aa1d-407a-d235-02da8290cdd1"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "158/158 [==============================] - 34s 213ms/step - loss: 2.4121 - accuracy: 0.9974 - val_loss: 0.0074 - val_accuracy: 0.9960\n",
            "Epoch 2/100\n",
            "158/158 [==============================] - 34s 217ms/step - loss: 2.7705 - accuracy: 0.9968 - val_loss: 0.0076 - val_accuracy: 0.9968\n",
            "Epoch 3/100\n",
            "158/158 [==============================] - 34s 217ms/step - loss: 2.6302 - accuracy: 0.9978 - val_loss: 0.0115 - val_accuracy: 0.9976\n",
            "Epoch 4/100\n",
            "158/158 [==============================] - 34s 217ms/step - loss: 2.2972 - accuracy: 0.9958 - val_loss: 0.0070 - val_accuracy: 0.9952\n",
            "Epoch 5/100\n",
            "158/158 [==============================] - 33s 210ms/step - loss: 1.9397 - accuracy: 0.9974 - val_loss: 0.0082 - val_accuracy: 0.9968\n",
            "Epoch 6/100\n",
            "158/158 [==============================] - 34s 216ms/step - loss: 1.8196 - accuracy: 0.9972 - val_loss: 0.0054 - val_accuracy: 0.9960\n",
            "Epoch 7/100\n",
            "158/158 [==============================] - 34s 216ms/step - loss: 1.5437 - accuracy: 0.9988 - val_loss: 0.0063 - val_accuracy: 0.9968\n",
            "Epoch 8/100\n",
            "158/158 [==============================] - 33s 211ms/step - loss: 1.5179 - accuracy: 0.9972 - val_loss: 0.0061 - val_accuracy: 0.9952\n",
            "Epoch 9/100\n",
            "158/158 [==============================] - 34s 216ms/step - loss: 1.5674 - accuracy: 0.9964 - val_loss: 0.0090 - val_accuracy: 0.9976\n",
            "Epoch 10/100\n",
            "158/158 [==============================] - 34s 217ms/step - loss: 1.8731 - accuracy: 0.9962 - val_loss: 0.0107 - val_accuracy: 0.9968\n",
            "Epoch 11/100\n",
            "158/158 [==============================] - 34s 216ms/step - loss: 2.1922 - accuracy: 0.9968 - val_loss: 0.0061 - val_accuracy: 0.9968\n",
            "Epoch 12/100\n",
            "158/158 [==============================] - 33s 211ms/step - loss: 2.0571 - accuracy: 0.9970 - val_loss: 0.0060 - val_accuracy: 0.9968\n",
            "Epoch 13/100\n",
            "158/158 [==============================] - 34s 214ms/step - loss: 2.0370 - accuracy: 0.9972 - val_loss: 0.0059 - val_accuracy: 0.9960\n",
            "Epoch 14/100\n",
            "158/158 [==============================] - 34s 218ms/step - loss: 1.9422 - accuracy: 0.9956 - val_loss: 0.0096 - val_accuracy: 0.9944\n",
            "Epoch 15/100\n",
            "158/158 [==============================] - 33s 212ms/step - loss: 1.9012 - accuracy: 0.9976 - val_loss: 0.0076 - val_accuracy: 0.9960\n",
            "Epoch 16/100\n",
            "158/158 [==============================] - 34s 214ms/step - loss: 1.8960 - accuracy: 0.9966 - val_loss: 0.0286 - val_accuracy: 0.9976\n",
            "Epoch 17/100\n",
            "158/158 [==============================] - 34s 215ms/step - loss: 2.4388 - accuracy: 0.9915 - val_loss: 0.0407 - val_accuracy: 0.9881\n",
            "Epoch 18/100\n",
            "158/158 [==============================] - 33s 210ms/step - loss: 2.7498 - accuracy: 0.9942 - val_loss: 0.0595 - val_accuracy: 0.9984\n",
            "Epoch 19/100\n",
            "158/158 [==============================] - 34s 215ms/step - loss: 2.8422 - accuracy: 0.9849 - val_loss: 0.0252 - val_accuracy: 0.9881\n",
            "Epoch 20/100\n",
            "158/158 [==============================] - 34s 214ms/step - loss: 2.1695 - accuracy: 0.9875 - val_loss: 0.0225 - val_accuracy: 0.9873\n",
            "Epoch 21/100\n",
            "158/158 [==============================] - 35s 219ms/step - loss: 1.6987 - accuracy: 0.9873 - val_loss: 0.0272 - val_accuracy: 0.9881\n",
            "Epoch 22/100\n",
            "158/158 [==============================] - 35s 222ms/step - loss: 1.5921 - accuracy: 0.9917 - val_loss: 0.0162 - val_accuracy: 0.9929\n",
            "Epoch 23/100\n",
            "158/158 [==============================] - 34s 217ms/step - loss: 1.3466 - accuracy: 0.9937 - val_loss: 0.0284 - val_accuracy: 0.9952\n",
            "Epoch 24/100\n",
            "158/158 [==============================] - 33s 210ms/step - loss: 1.5979 - accuracy: 0.9968 - val_loss: 0.0092 - val_accuracy: 0.9960\n",
            "Epoch 25/100\n",
            "158/158 [==============================] - 34s 214ms/step - loss: 1.6631 - accuracy: 0.9968 - val_loss: 0.0093 - val_accuracy: 0.9960\n",
            "Epoch 26/100\n",
            "158/158 [==============================] - 34s 214ms/step - loss: 1.6817 - accuracy: 0.9962 - val_loss: 0.0084 - val_accuracy: 0.9976\n",
            "Epoch 27/100\n",
            "158/158 [==============================] - 33s 209ms/step - loss: 1.8467 - accuracy: 0.9986 - val_loss: 0.0212 - val_accuracy: 0.9952\n",
            "Epoch 28/100\n",
            "158/158 [==============================] - 34s 212ms/step - loss: 2.4160 - accuracy: 0.9972 - val_loss: 0.0125 - val_accuracy: 0.9952\n",
            "Epoch 29/100\n",
            "158/158 [==============================] - 33s 210ms/step - loss: 2.5944 - accuracy: 0.9950 - val_loss: 0.0085 - val_accuracy: 0.9968\n",
            "Epoch 30/100\n",
            "158/158 [==============================] - 33s 206ms/step - loss: 2.3936 - accuracy: 0.9968 - val_loss: 0.0106 - val_accuracy: 0.9968\n",
            "Epoch 31/100\n",
            "158/158 [==============================] - 33s 212ms/step - loss: 2.4093 - accuracy: 0.9966 - val_loss: 0.0092 - val_accuracy: 0.9984\n",
            "Epoch 32/100\n",
            "158/158 [==============================] - 33s 211ms/step - loss: 2.0727 - accuracy: 0.9954 - val_loss: 0.0067 - val_accuracy: 0.9952\n",
            "Epoch 33/100\n",
            "158/158 [==============================] - 32s 205ms/step - loss: 1.6653 - accuracy: 0.9978 - val_loss: 0.0061 - val_accuracy: 0.9984\n",
            "Epoch 34/100\n",
            "158/158 [==============================] - 33s 210ms/step - loss: 1.4999 - accuracy: 0.9933 - val_loss: 0.0145 - val_accuracy: 0.9944\n",
            "Epoch 35/100\n",
            "158/158 [==============================] - 33s 210ms/step - loss: 1.5218 - accuracy: 0.9968 - val_loss: 0.0116 - val_accuracy: 0.9968\n",
            "Epoch 36/100\n",
            "158/158 [==============================] - 32s 206ms/step - loss: 1.5110 - accuracy: 0.9956 - val_loss: 0.0133 - val_accuracy: 0.9960\n",
            "Epoch 37/100\n",
            "158/158 [==============================] - 33s 210ms/step - loss: 1.7987 - accuracy: 0.9970 - val_loss: 0.0096 - val_accuracy: 0.9968\n",
            "Epoch 38/100\n",
            "158/158 [==============================] - 34s 212ms/step - loss: 2.2629 - accuracy: 0.9956 - val_loss: 0.0082 - val_accuracy: 0.9952\n",
            "Epoch 39/100\n",
            "158/158 [==============================] - 33s 206ms/step - loss: 2.4366 - accuracy: 0.9984 - val_loss: 0.0078 - val_accuracy: 0.9960\n",
            "Epoch 40/100\n",
            "158/158 [==============================] - 33s 210ms/step - loss: 2.3229 - accuracy: 0.9964 - val_loss: 0.0150 - val_accuracy: 0.9968\n",
            "Epoch 41/100\n",
            "158/158 [==============================] - 33s 212ms/step - loss: 2.3211 - accuracy: 0.9982 - val_loss: 0.0273 - val_accuracy: 0.9960\n",
            "Epoch 42/100\n",
            "158/158 [==============================] - 33s 206ms/step - loss: 2.3702 - accuracy: 0.9952 - val_loss: 0.0140 - val_accuracy: 0.9929\n",
            "Epoch 43/100\n",
            "158/158 [==============================] - 33s 209ms/step - loss: 2.2507 - accuracy: 0.9970 - val_loss: 0.0137 - val_accuracy: 0.9944\n",
            "Epoch 44/100\n",
            "158/158 [==============================] - 33s 209ms/step - loss: 2.0114 - accuracy: 0.9962 - val_loss: 0.0066 - val_accuracy: 0.9960\n",
            "Epoch 45/100\n",
            "158/158 [==============================] - 33s 206ms/step - loss: 1.4802 - accuracy: 0.9976 - val_loss: 0.0060 - val_accuracy: 0.9984\n",
            "Epoch 46/100\n",
            "158/158 [==============================] - 33s 211ms/step - loss: 1.3821 - accuracy: 0.9974 - val_loss: 0.0054 - val_accuracy: 0.9968\n",
            "Epoch 47/100\n",
            "158/158 [==============================] - 33s 212ms/step - loss: 1.5048 - accuracy: 0.9982 - val_loss: 0.0061 - val_accuracy: 0.9984\n",
            "Epoch 48/100\n",
            "158/158 [==============================] - 33s 206ms/step - loss: 1.5966 - accuracy: 0.9976 - val_loss: 0.0145 - val_accuracy: 0.9944\n",
            "Epoch 49/100\n",
            "158/158 [==============================] - 34s 213ms/step - loss: 2.0833 - accuracy: 0.9984 - val_loss: 0.0286 - val_accuracy: 0.9960\n",
            "Epoch 50/100\n",
            "158/158 [==============================] - 33s 211ms/step - loss: 2.6340 - accuracy: 0.9933 - val_loss: 0.0367 - val_accuracy: 0.9889\n",
            "Epoch 51/100\n",
            "158/158 [==============================] - 33s 207ms/step - loss: 2.8042 - accuracy: 0.9974 - val_loss: 0.0085 - val_accuracy: 0.9960\n",
            "Epoch 52/100\n",
            "158/158 [==============================] - 33s 211ms/step - loss: 2.2150 - accuracy: 0.9968 - val_loss: 0.0115 - val_accuracy: 0.9960\n",
            "Epoch 53/100\n",
            "158/158 [==============================] - 33s 209ms/step - loss: 2.0083 - accuracy: 0.9970 - val_loss: 0.0159 - val_accuracy: 0.9921\n",
            "Epoch 54/100\n",
            "158/158 [==============================] - 32s 202ms/step - loss: 1.8169 - accuracy: 0.9970 - val_loss: 0.0056 - val_accuracy: 0.9960\n",
            "Epoch 55/100\n",
            "158/158 [==============================] - 33s 208ms/step - loss: 1.3678 - accuracy: 0.9986 - val_loss: 0.0148 - val_accuracy: 0.9968\n",
            "Epoch 56/100\n",
            "158/158 [==============================] - 33s 207ms/step - loss: 1.3676 - accuracy: 0.9972 - val_loss: 0.0056 - val_accuracy: 0.9952\n",
            "Epoch 57/100\n",
            "158/158 [==============================] - 32s 202ms/step - loss: 1.1788 - accuracy: 0.9980 - val_loss: 0.0061 - val_accuracy: 0.9976\n",
            "Epoch 58/100\n",
            "158/158 [==============================] - 33s 208ms/step - loss: 1.4088 - accuracy: 0.9966 - val_loss: 0.0058 - val_accuracy: 0.9952\n",
            "Epoch 59/100\n",
            "158/158 [==============================] - 33s 208ms/step - loss: 1.6861 - accuracy: 0.9982 - val_loss: 0.0074 - val_accuracy: 0.9968\n",
            "Epoch 60/100\n",
            "158/158 [==============================] - 32s 203ms/step - loss: 2.5451 - accuracy: 0.9976 - val_loss: 0.0089 - val_accuracy: 0.9960\n",
            "Epoch 61/100\n",
            "158/158 [==============================] - 33s 207ms/step - loss: 2.7983 - accuracy: 0.9970 - val_loss: 0.0057 - val_accuracy: 0.9976\n",
            "Epoch 62/100\n",
            "158/158 [==============================] - 33s 207ms/step - loss: 2.5728 - accuracy: 0.9966 - val_loss: 0.0369 - val_accuracy: 0.9984\n",
            "Epoch 63/100\n",
            "158/158 [==============================] - 32s 203ms/step - loss: 2.6130 - accuracy: 0.9960 - val_loss: 0.0075 - val_accuracy: 0.9976\n",
            "Epoch 64/100\n",
            "158/158 [==============================] - 33s 207ms/step - loss: 2.2335 - accuracy: 0.9982 - val_loss: 0.0312 - val_accuracy: 0.9825\n",
            "Epoch 65/100\n",
            "158/158 [==============================] - 33s 208ms/step - loss: 2.1964 - accuracy: 0.9960 - val_loss: 0.0120 - val_accuracy: 0.9960\n",
            "Epoch 66/100\n",
            "158/158 [==============================] - 32s 203ms/step - loss: 1.9352 - accuracy: 0.9960 - val_loss: 0.0070 - val_accuracy: 0.9968\n",
            "Epoch 67/100\n",
            "158/158 [==============================] - 33s 208ms/step - loss: 1.4362 - accuracy: 0.9976 - val_loss: 0.0223 - val_accuracy: 0.9944\n",
            "Epoch 68/100\n",
            "158/158 [==============================] - 32s 204ms/step - loss: 1.4076 - accuracy: 0.9966 - val_loss: 0.0118 - val_accuracy: 0.9976\n",
            "Epoch 69/100\n",
            "158/158 [==============================] - 33s 210ms/step - loss: 1.5431 - accuracy: 0.9984 - val_loss: 0.0163 - val_accuracy: 0.9968\n",
            "Epoch 70/100\n",
            "158/158 [==============================] - 33s 210ms/step - loss: 1.8589 - accuracy: 0.9978 - val_loss: 0.0132 - val_accuracy: 0.9968\n",
            "Epoch 71/100\n",
            "158/158 [==============================] - 32s 206ms/step - loss: 2.2185 - accuracy: 0.9964 - val_loss: 0.0070 - val_accuracy: 0.9984\n",
            "Epoch 72/100\n",
            "158/158 [==============================] - 33s 208ms/step - loss: 2.0319 - accuracy: 0.9958 - val_loss: 0.0068 - val_accuracy: 0.9968\n",
            "Epoch 73/100\n",
            "158/158 [==============================] - 33s 208ms/step - loss: 1.9797 - accuracy: 0.9988 - val_loss: 0.0068 - val_accuracy: 0.9984\n",
            "Epoch 74/100\n",
            "158/158 [==============================] - 32s 205ms/step - loss: 1.8924 - accuracy: 0.9980 - val_loss: 0.0251 - val_accuracy: 0.9937\n",
            "Epoch 75/100\n",
            "158/158 [==============================] - 33s 209ms/step - loss: 2.2288 - accuracy: 0.9970 - val_loss: 0.0134 - val_accuracy: 0.9968\n",
            "Epoch 76/100\n",
            "158/158 [==============================] - 33s 210ms/step - loss: 2.3253 - accuracy: 0.9978 - val_loss: 0.0112 - val_accuracy: 0.9944\n",
            "Epoch 77/100\n",
            "158/158 [==============================] - 32s 205ms/step - loss: 2.4160 - accuracy: 0.9972 - val_loss: 0.0105 - val_accuracy: 0.9952\n",
            "Epoch 78/100\n",
            "158/158 [==============================] - 33s 211ms/step - loss: 2.0935 - accuracy: 0.9976 - val_loss: 0.0210 - val_accuracy: 0.9952\n",
            "Epoch 79/100\n",
            "158/158 [==============================] - 33s 212ms/step - loss: 2.1036 - accuracy: 0.9978 - val_loss: 0.0893 - val_accuracy: 0.9222\n",
            "Epoch 80/100\n",
            "158/158 [==============================] - 32s 205ms/step - loss: 2.0695 - accuracy: 0.9966 - val_loss: 0.0105 - val_accuracy: 0.9952\n",
            "Epoch 81/100\n",
            "158/158 [==============================] - 33s 209ms/step - loss: 1.8747 - accuracy: 0.9976 - val_loss: 0.0082 - val_accuracy: 0.9960\n",
            "Epoch 82/100\n",
            "158/158 [==============================] - 33s 209ms/step - loss: 1.5065 - accuracy: 0.9966 - val_loss: 0.0066 - val_accuracy: 0.9968\n",
            "Epoch 83/100\n",
            "158/158 [==============================] - 32s 204ms/step - loss: 1.3533 - accuracy: 0.9988 - val_loss: 0.0082 - val_accuracy: 0.9976\n",
            "Epoch 84/100\n",
            "158/158 [==============================] - 33s 209ms/step - loss: 1.4641 - accuracy: 0.9972 - val_loss: 0.0066 - val_accuracy: 0.9968\n",
            "Epoch 85/100\n",
            "158/158 [==============================] - 33s 210ms/step - loss: 1.6152 - accuracy: 0.9976 - val_loss: 0.0083 - val_accuracy: 0.9968\n",
            "Epoch 86/100\n",
            "158/158 [==============================] - 32s 205ms/step - loss: 1.9614 - accuracy: 0.9980 - val_loss: 0.0173 - val_accuracy: 0.9976\n",
            "Epoch 87/100\n",
            "158/158 [==============================] - 33s 212ms/step - loss: 2.6303 - accuracy: 0.9972 - val_loss: 0.0099 - val_accuracy: 0.9968\n",
            "Epoch 88/100\n",
            "158/158 [==============================] - 32s 204ms/step - loss: 2.7015 - accuracy: 0.9964 - val_loss: 0.0121 - val_accuracy: 0.9952\n",
            "Epoch 89/100\n",
            "158/158 [==============================] - 33s 210ms/step - loss: 2.5804 - accuracy: 0.9978 - val_loss: 0.0076 - val_accuracy: 0.9968\n",
            "Epoch 90/100\n",
            "158/158 [==============================] - 32s 205ms/step - loss: 2.2086 - accuracy: 0.9970 - val_loss: 0.0068 - val_accuracy: 0.9960\n",
            "Epoch 91/100\n",
            "158/158 [==============================] - 33s 212ms/step - loss: 1.8963 - accuracy: 0.9988 - val_loss: 0.0070 - val_accuracy: 0.9960\n",
            "Epoch 92/100\n",
            "158/158 [==============================] - 33s 211ms/step - loss: 1.6618 - accuracy: 0.9978 - val_loss: 0.0183 - val_accuracy: 0.9968\n",
            "Epoch 93/100\n",
            "158/158 [==============================] - 33s 206ms/step - loss: 1.7644 - accuracy: 0.9970 - val_loss: 0.0218 - val_accuracy: 0.9952\n",
            "Epoch 94/100\n",
            "158/158 [==============================] - 33s 209ms/step - loss: 1.8813 - accuracy: 0.9978 - val_loss: 0.0236 - val_accuracy: 0.9929\n",
            "Epoch 95/100\n",
            "158/158 [==============================] - 33s 210ms/step - loss: 2.0195 - accuracy: 0.9962 - val_loss: 0.0174 - val_accuracy: 0.9968\n",
            "Epoch 96/100\n",
            "158/158 [==============================] - 32s 204ms/step - loss: 2.0812 - accuracy: 0.9980 - val_loss: 0.0067 - val_accuracy: 0.9976\n",
            "Epoch 97/100\n",
            "158/158 [==============================] - 33s 210ms/step - loss: 1.8323 - accuracy: 0.9948 - val_loss: 0.0107 - val_accuracy: 0.9960\n",
            "Epoch 98/100\n",
            "158/158 [==============================] - 33s 208ms/step - loss: 1.7686 - accuracy: 0.9958 - val_loss: 0.0091 - val_accuracy: 0.9960\n",
            "Epoch 99/100\n",
            "158/158 [==============================] - 32s 204ms/step - loss: 1.6577 - accuracy: 0.9976 - val_loss: 0.0097 - val_accuracy: 0.9960\n",
            "Epoch 100/100\n",
            "158/158 [==============================] - 33s 208ms/step - loss: 1.7759 - accuracy: 0.9972 - val_loss: 0.0099 - val_accuracy: 0.9960\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xtetest = grids\n",
        "x2tetest = fourpredictienn\n",
        "test_input = np.array(xtetest).reshape(len(xtetest), dimgrids,dimgrids,1)\n",
        "test2_input = np.array(x2tetest).reshape(len(x2tetest),9)\n",
        "test_output = np.array(modelhid.predict((test_input,test2_input), verbose=0))\n",
        "hidpredictienn = (test_output)"
      ],
      "metadata": {
        "id": "5WFgZRRjOvhi"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# From estimated parameters back to prediction"
      ],
      "metadata": {
        "id": "r3PTnuockA9J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fourpred = fourpredictienn\n",
        "hiddenpred = hidpredictienn"
      ],
      "metadata": {
        "id": "ZgVdY902kD88"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "allefourpredtussen = []\n",
        "for jj in range(len(topred)):\n",
        "  toetevoegen = np.zeros((99,2))\n",
        "  for ii in range(imlim[0],imlim[1]):\n",
        "    toetevoegen[ii][1] = 0\n",
        "  for ii in range(relim[0],relim[1]):\n",
        "    toetevoegen[ii][0] = 0\n",
        "  for ii in range(0,relim[0]):\n",
        "    toetevoegen[ii][0] = fourpred[jj,ii]\n",
        "  for ii in range(0,imlim[0]):\n",
        "    toetevoegen[ii][1] = fourpred[jj,ii+5]\n",
        "  allefourpredtussen.append(toetevoegen)\n",
        "\n",
        "allefourpred = allefourpredtussen.copy()\n",
        "for jj in range(len(topred)):\n",
        "  for ii in range(imlim[1],imlim[2]):\n",
        "    allefourpred[jj][ii][1] = -allefourpredtussen[jj][np.int((dim_targetdata+1)/2-abs((dim_targetdata-1)/2-ii))][1]\n",
        "  for ii in range(relim[1],relim[2]):\n",
        "    allefourpred[jj][ii][0] = allefourpredtussen[jj][np.int((dim_targetdata+1)/2-abs((dim_targetdata-1)/2-ii))][0]  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BYdgp3Q3kKMm",
        "outputId": "75e27df7-ca5c-4a5f-c6c3-0beb1f3b9aa2"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:17: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:19: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "allefourpredcompl = []\n",
        "for jj in range(len(allefourpred)):\n",
        "  tijdsvector = []\n",
        "  for ii in range(len(allefourpred[0])):\n",
        "      tijdsvector.append(complex(allefourpred[jj][ii][0],allefourpred[jj][ii][1]))\n",
        "  allefourpredcompl.append(tijdsvector)"
      ],
      "metadata": {
        "id": "MCj0B0eOkM76"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inverse fouriertransform"
      ],
      "metadata": {
        "id": "XHqU60AaxAJq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tijdslijntjenafour = np.real(sc.fft.ifft(allefourpredcompl))"
      ],
      "metadata": {
        "id": "0_kp-AdLkOiQ"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adding the auto-encoder"
      ],
      "metadata": {
        "id": "xUO8lhM_kQ8G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Xen = np.array(hiddenpred).reshape(len(hiddenpred), dim_latent_space)\n",
        "layer_output = modelauto.get_layer('dense_3').output #last layer\n",
        "layer_input = modelauto.get_layer('dense_2').output #first dense layer\n",
        "automoduit =tf.keras.models.Model(inputs=layer_input,outputs=layer_output)\n",
        "correctiepred=automoduit.predict(Xen) # = hidden vectors!"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WBWwq6j3kTf5",
        "outputId": "8a9c534b-a4f7-4486-9e2a-8a62916d674b"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "230/230 [==============================] - 0s 839us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "finalepred = correctiepred + tijdslijntjenafour"
      ],
      "metadata": {
        "id": "tLQe5qyvkVM1"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(finalepred[900],'.',label='Predicted timeline')\n",
        "plt.plot(targetdata[900],'.',label='Real timeline')\n",
        "plt.legend()\n",
        "plt.xlabel('Time')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "axuhxvOBxQ3Y",
        "outputId": "cfe7e13c-7fcd-4a10-9c23-c6338dae0345"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 0, 'Time')"
            ]
          },
          "metadata": {},
          "execution_count": 51
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(modinfo[:,0],finalepred[:,30],'.',label='Predicted value at 50th time')\n",
        "plt.plot(modinfo[:,0],targetdata[:,30],'.',alpha=0.5,label='Real value at 50th time')\n",
        "plt.legend()\n",
        "plt.xlabel('Depth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "EgApFpT2xSpi",
        "outputId": "dee66bfb-53bf-4d45-8d70-eef4050898f0"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 0, 'Depth')"
            ]
          },
          "metadata": {},
          "execution_count": 53
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}