{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import the libraries that are needed"
      ],
      "metadata": {
        "id": "gN35Ae16c7Z7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6MK6jzNccegg"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import seaborn as sns\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import sklearn\n",
        "from sklearn import preprocessing\n",
        "import scipy as sc\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.compose import ColumnTransformer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#initialize the train and test data set + perform the fourier transformation"
      ],
      "metadata": {
        "id": "IBHRRToidq-L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "targetdata =  #data that we want to predict\n",
        "dim_targetdata = #number of values that need to be predicted per input point\n",
        "features = #input data for the model\n",
        "dim_modinfo = #number of input parameters per input point\n",
        "train_size = #number of datapoints in the train dataset\n",
        "test_size = #number of datapoints in the test dataset\n",
        "dim_latent_space = #dimension of the latent space representation\n",
        "num_feature_maps = #number of feature maps in the convolutional layer used to predict the parameters (optimization might yield different parameters for the fouriercomponents and the hidden states but that is not taken into account here)\n",
        "kernel_size = #dimension of the kernel in the convolutional layer used to predict the parameters(kernel_size x kernel_size) (optimization might yield different parameters for the fouriercomponents and the hidden states but that is not taken into account here)\n",
        "dense_layer_size = #number of units for the dense layer (optimization might yield different parameters for the fouriercomponents and the hidden states but that is not taken into account here)\n",
        "grids = #input figures\n",
        "dimgrids = #dimensionality of the grids"
      ],
      "metadata": {
        "id": "v2lisNhXdxst"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#perform the fourier transform\n",
        "verschilfour = []\n",
        "for ii in range(len(targetdata)):\n",
        "  verschilfour.append(sc.fft.fft(targetdata[ii]))\n",
        "verschilfour = np.array(verschilfour)\n",
        "#split in real and imaginary part\n",
        "re = np.real(verschilfour)\n",
        "im = np.imag(verschilfour)\n",
        "topred = []\n",
        "for ii in range(len(re)):\n",
        "  tijdsding = []\n",
        "  for jj in range(len(re[0])):\n",
        "    elnt = [re[ii,jj],im[ii,jj]]\n",
        "    tijdsding.append(elnt)\n",
        "  topred.append(tijdsding)\n",
        "topred = np.array(topred)\n",
        "\n",
        "indexen = range(len(verschilfour))\n",
        "\n",
        "#split in train and test data set\n",
        "X_trainges, X_testges, indexen_trainges_final, indexen_testges_final = train_test_split(features, indexen, train_size=train_size, test_size=test_size, random_state=333)\n",
        "training_yges = topred[indexen_trainges_final]\n",
        "y_testges = topred[indexen_testges_final]"
      ],
      "metadata": {
        "id": "MSxgPAf7c_qK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exploit symmetry of the fouriertransform"
      ],
      "metadata": {
        "id": "n-Z65cUhfIwp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "redeel = (topred[:,:,0]).copy()\n",
        "for ii in range(int((dim_targetdata-1)/2),dim_targetdata):\n",
        "  redeel[:,ii] = topred[:,int((dim_targetdata+1)/2-abs((dim_targetdata-1)/2-ii)),0]"
      ],
      "metadata": {
        "id": "_sCyJZ9ZfOL6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imdeel = (topred[:,:,1]).copy()\n",
        "for ii in range(int((dim_targetdata-1)/2),dim_targetdata):\n",
        "  imdeel[:,ii] = -topred[:,int((dim_targetdata+1)/2-abs((dim_targetdata-1)/2-ii)),1]"
      ],
      "metadata": {
        "id": "5bh_Rq_qfuk4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define the fouriercomponents that will be used for the prediction"
      ],
      "metadata": {
        "id": "kFQW9UcVf8QC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "relim = #[x,y,z]: fouriercomponents from 0 to x and from y to z are taken into account for the real part with the last ones mirrored from the first ones\n",
        "imlim = #[x,y,z]: fouriercomponents from 0 to x and from y to z are taken into account for the imag part with the last ones mirrored from the first ones"
      ],
      "metadata": {
        "id": "btkWmqTvgFbF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predfour = topred.copy()\n",
        "for jj in range(len(predfour)):\n",
        "  for ii in range(imlim[0],imlim[1]):\n",
        "    predfour[jj][ii][1] = 0\n",
        "  for ii in range(relim[0],relim[1]):\n",
        "    predfour[jj][ii][0] = 0\n",
        "  for ii in range(imlim[1],imlim[2]):\n",
        "    predfour[jj][ii][1] = -topred[jj,np.int((dim_targetdata+1)/2-abs((dim_targetdata-1)/2-ii)),1]\n",
        "  for ii in range(relim[1],relim[2]):\n",
        "    predfour[jj][ii][0] = topred[jj,np.int((dim_targetdata+1)/2-abs((dim_targetdata-1)/2-ii)),0]  "
      ],
      "metadata": {
        "id": "_ISSR_MmgBTx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prediction based on the limited number of fourier components"
      ],
      "metadata": {
        "id": "QpNmnNKJgbJu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pred = []\n",
        "for ii in range(len(predfour)):\n",
        "  tijdsvector = []\n",
        "  for jj in range(len(predfour[0])):\n",
        "    tijdsvector.append(complex(predfour[ii,jj,0],predfour[ii,jj,1]))\n",
        "  pred.append(tijdsvector)\n",
        "predictie = np.real(sc.fft.ifft(pred))"
      ],
      "metadata": {
        "id": "XiqBSnDegeXl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Use neural network to estimate the difference between the fourierprediction and the real values"
      ],
      "metadata": {
        "id": "IsfpWqB2gpCB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "errfour = []\n",
        "for ii in range(len(targetdata)):\n",
        "  errfour.append(targetdata[ii]-predictie[ii])"
      ],
      "metadata": {
        "id": "cUerv673gt8x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "split in train and test dataset"
      ],
      "metadata": {
        "id": "cFpXXkD1hDGK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "indexenfourr = range(len(errfour))\n",
        "errfour_train, errfour_test, indexen_trainges, indexen_testges = train_test_split(errfour, indexenfourr, train_size=train_size, test_size=test_size, random_state=333)"
      ],
      "metadata": {
        "id": "UXbYVuGxhFIg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "make and train the neural network"
      ],
      "metadata": {
        "id": "QcXtsD1dhLdq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.array(errfour_train).reshape(len(errfour_train), dim_targetdata)\n",
        "params_input = keras.Input(shape=(dim_targetdata))\n",
        "encoder = tf.keras.layers.Dense(dim_latent_space,activation = 'tanh')(params_input)\n",
        "decoder = tf.keras.layers.Dense(dim_targetdata)(encoder)\n",
        "modelauto = keras.Model(inputs=params_input,outputs=decoder)\n",
        "modelauto.compile(optimizer='adam', loss='mse')\n",
        "modelauto.summary()\n",
        "hist = modelauto.fit(X,X,epochs=500,validation_split=0.2,batch_size=32)\n",
        "plt.semilogy(hist.history['loss'])"
      ],
      "metadata": {
        "id": "F850WGw5hLJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prediction based on limited number of fourier components and predicted difference"
      ],
      "metadata": {
        "id": "U7AadO2Jhfo6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_input = np.array(errfour).reshape(len(errfour), dim_targetdata)\n",
        "test_output = np.array(modelauto.predict(test_input, verbose=0)) # = echt - fourier"
      ],
      "metadata": {
        "id": "uuhXocmShus5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "uiteindelijkeschatting = test_output + predictie"
      ],
      "metadata": {
        "id": "sm_beURKhgRq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prediction of the parameters"
      ],
      "metadata": {
        "id": "1t9u9HWOhymf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fourtopred = np.concatenate([predfour[:,0:5,0],predfour[:,1:4,1]],axis = 1) # 1:4 because first one is always 0\n",
        "#obtain the hidden vectors\n",
        "Xen = np.array(errfour).reshape(len(errfour), dim_targetdata)\n",
        "layer_output = modelauto.get_layer('dense').output #dense can need an extra number, depending on how much the algorithm is performed\n",
        "intermediate_model=tf.keras.models.Model(inputs=params_input,outputs=layer_output)\n",
        "tussen=intermediate_model.predict(Xen) # = hidden vectors!\n",
        "tevoorspellen = np.concatenate([fourtopred,tussen],axis = 1)"
      ],
      "metadata": {
        "id": "QkWZoZnch0lW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "indexen = range(len(features))\n",
        "X_train_four, X_test_four, indexen_train_four, indexen_test_four = train_test_split(grids, indexen, train_size=train_size, test_size=test_size, random_state=333)\n",
        "training_y_four = fourtopred[indexen_train_four]\n",
        "y_test_four = fourtopred[indexen_test_four]\n",
        "\n",
        "X = np.array(X_train_four).reshape(len(X_train_four), dimgrids,dimgrids,1)\n",
        "\n",
        "Y = np.array(training_y_four).reshape(len(training_y_four), 8)\n",
        "#using a standardscaler here is advised"
      ],
      "metadata": {
        "id": "J9pXpZTLii0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make and train the neural network"
      ],
      "metadata": {
        "id": "HuD7mQsWjRNl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "modelsfour = Sequential()\n",
        "modelsfour.add(Conv2D(num_feature_maps,kernel_size=kernel_size,activation='relu',input_shape=(dimgrids, dimgrids, 1)))\n",
        "modelsfour.add(MaxPooling2D(2, 2))\n",
        "modelsfour.add(Flatten())\n",
        "modelsfour.add(Dense(dense_layer_size, activation='relu'))\n",
        "modelsfour.add(Dropout(0.2))\n",
        "modelsfour.add(Dense(8))\n",
        "modelsfour.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"accuracy\"])\n",
        "modelsfour.summary()\n",
        "hist = modelfour.fit(X,Y,epochs=1000,validation_split=0.2,batch_size=32) "
      ],
      "metadata": {
        "id": "UayXGd4vMc86"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_input = np.array(grids).reshape(len(grids), dimgrids,dimgrids,1)\n",
        "test_output = np.array(modelfour.predict(test_input, verbose=0))\n",
        "fourpredictienn = np.vstack(np.transpose(np.array((test_output[:,0],test_output[:,1],test_output[:,2],test_output[:,3],test_output[:,4],np.zeros(len(test_output)),test_output[:,5],test_output[:,6],test_output[:,7]))))"
      ],
      "metadata": {
        "id": "_pAk4Vpqjnbx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Predict the hidden state"
      ],
      "metadata": {
        "id": "l3kTVUZJjy6h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "indexen = range(len(features))\n",
        "feathid = []\n",
        "#add the predicted fouriercomponents to the input of hiddenstate prediction\n",
        "for ii in range(len(features)):\n",
        "  feathid.append(np.concatenate([features[ii], fourpredictienn[ii]]))\n",
        "\n",
        "\n",
        "X_train_hid, X_test_hid, indexen_train_hid, indexen_test_hid = train_test_split(grids, indexen, train_size= train_size, test_size=test_size, random_state=333)\n",
        "training_y_hid = tussen[indexen_train_hid]\n",
        "y_test_hid = tussen[indexen_test_hid]\n",
        "X2_train = fourpredictienn[indexen_train_hid]\n",
        "X2_test = fourpredictienn[indexen_test_hid]\n",
        "\n",
        "X1 = np.array(X_train_hid).reshape(len(X_train_hid), dimgrids,dimgrids,1)\n",
        "X2 = np.array(X2_train).reshape(len(X2_train),9)\n",
        "\n",
        "Y = np.array(training_y_hid).reshape(len(training_y_hid), 5)\n",
        "#again it's better to standardscale"
      ],
      "metadata": {
        "id": "pdB1fiBmNocx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inp1 = keras.Input(shape=(dimgrids,dimgrids,1))\n",
        "inp2 = keras.Input(shape=(9))\n",
        "convlayer = Conv2D(num_feature_maps,kernel_size=kernel_size,activation='relu',input_shape=(dimgrids, dimgrids, 1))(inp1)\n",
        "maxpool = MaxPooling2D(2,2)(convlayer)\n",
        "flatlay = Flatten()(maxpool)\n",
        "denselay = Dense(dense_layer_size, activation='relu')(flatlay)\n",
        "droplay = Dropout(0.2)(denselay)\n",
        "conclay = layers.concatenate([droplay,inp2])\n",
        "denselas = (Dense(5))(conclay)\n",
        "modelhid = keras.Model(inputs=(inp1,inp2),outputs=denselas)\n",
        "modelhid.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"accuracy\"])\n",
        "modelhid.summary()"
      ],
      "metadata": {
        "id": "6RHTy0UxNoR5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hist = modelhid.fit((X1,X2),Y,epochs=500,validation_split=0.2,batch_size=32)"
      ],
      "metadata": {
        "id": "EQ-qSkAzOdHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xtetest = grids\n",
        "x2tetest = fourpredictienn\n",
        "test_input = np.array(xtetest).reshape(len(xtetest), dimgrids,dimgrids,1)\n",
        "test2_input = np.array(x2tetest).reshape(len(x2tetest),9)\n",
        "test_output = np.array(modelhid.predict((test_input,test2_input), verbose=0))\n",
        "hidpredictienn = transtarg.inverse_transform(test_output)"
      ],
      "metadata": {
        "id": "5WFgZRRjOvhi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# From estimated parameters back to prediction"
      ],
      "metadata": {
        "id": "r3PTnuockA9J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fourpred = fourpredictienn\n",
        "hiddenpred = hidpredictienn"
      ],
      "metadata": {
        "id": "ZgVdY902kD88"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "allefourpredtussen = []\n",
        "for jj in range(len(topred)):\n",
        "  toetevoegen = np.zeros((99,2))\n",
        "  for ii in range(imlim[0],imlim[1]):\n",
        "    toetevoegen[ii][1] = 0\n",
        "  for ii in range(relim[0],relim[1]):\n",
        "    toetevoegen[ii][0] = 0\n",
        "  for ii in range(0,relim[0]):\n",
        "    toetevoegen[ii][0] = fourpred[jj,ii]\n",
        "  for ii in range(0,imlim[0]):\n",
        "    toetevoegen[ii][1] = fourpred[jj,ii+5]\n",
        "  allefourpredtussen.append(toetevoegen)\n",
        "\n",
        "allefourpred = allefourpredtussen.copy()\n",
        "for jj in range(len(topred)):\n",
        "  for ii in range(imlim[1],imlim[2]):\n",
        "    allefourpred[jj][ii][1] = -allefourpredtussen[jj][np.int((dim_targetdata+1)/2-abs((dim_targetdata-1)/2-ii))][1]\n",
        "  for ii in range(relim[1],relim[2]):\n",
        "    allefourpred[jj][ii][0] = allefourpredtussen[jj][np.int((dim_targetdata+1)/2-abs((dim_targetdata-1)/2-ii))][0]  "
      ],
      "metadata": {
        "id": "BYdgp3Q3kKMm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "allefourpredcompl = []\n",
        "for jj in range(len(allefourpred)):\n",
        "  tijdsvector = []\n",
        "  for ii in range(len(allefourpred[0])):\n",
        "      tijdsvector.append(complex(allefourpred[jj][ii][0],allefourpred[jj][ii][1]))\n",
        "  allefourpredcompl.append(tijdsvector)"
      ],
      "metadata": {
        "id": "MCj0B0eOkM76"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inverse fouriertransform"
      ],
      "metadata": {
        "id": "XHqU60AaxAJq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tijdslijntjenafour = np.real(sc.fft.ifft(allefourpredcompl))"
      ],
      "metadata": {
        "id": "0_kp-AdLkOiQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adding the auto-encoder"
      ],
      "metadata": {
        "id": "xUO8lhM_kQ8G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Xen = np.array(hiddenpred).reshape(len(hiddenpred), dim_latent_space)\n",
        "layer_output = modelauto.get_layer('dense_1').output #last laag\n",
        "layer_input = modelauto.get_layer('dense').output #first dense layer\n",
        "automoduit =tf.keras.models.Model(inputs=layer_input,outputs=layer_output)\n",
        "correctiepred=automoduit.predict(Xen) # = hidden vectors!"
      ],
      "metadata": {
        "id": "WBWwq6j3kTf5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "finalepred = correctiepred + tijdslijntjenafour"
      ],
      "metadata": {
        "id": "tLQe5qyvkVM1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}